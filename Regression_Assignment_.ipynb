{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.What is Simple Linear Regression ?\n",
        "# **Ans:-** Simple Linear Regression:\n",
        "\n",
        "**Simple Linear Regression** is a statistical method used to model the relationship between two variables:  \n",
        "1. **Independent Variable (Predictor/Explanatory Variable)**: Denoted as \\( X \\).  \n",
        "2. **Dependent Variable (Response/Outcome Variable)**: Denoted as \\( Y \\).\n",
        "\n",
        "The goal is to find a linear equation that best predicts \\( Y \\) based on \\( X \\).\n",
        "\n",
        "---\n",
        "\n",
        "### The Equation:\n",
        "\\[\n",
        "Y = \\beta_0 + \\beta_1X + \\epsilon\n",
        "\\]\n",
        "Where:  \n",
        "- \\( Y \\): Dependent variable (predicted value).  \n",
        "- \\( X \\): Independent variable.  \n",
        "- \\( \\beta_0 \\): Intercept (value of \\( Y \\) when \\( X = 0 \\)).  \n",
        "- \\( \\beta_1 \\): Slope (change in \\( Y \\) for a unit change in \\( X \\)).  \n",
        "- \\( \\epsilon \\): Error term (difference between observed and predicted \\( Y \\)).\n",
        "\n",
        "---\n",
        "\n",
        "### Assumptions:\n",
        "1. **Linearity**: The relationship between \\( X \\) and \\( Y \\) is linear.  \n",
        "2. **Independence**: Observations are independent.  \n",
        "3. **Homoscedasticity**: The variance of residuals (errors) is constant across all values of \\( X \\).  \n",
        "4. **Normality**: Residuals are normally distributed.\n",
        "\n",
        "---\n",
        "\n",
        "### Objective:\n",
        "Minimize the **Sum of Squared Errors (SSE)**:  \n",
        "\\[\n",
        "SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\n",
        "\\]  \n",
        "Where \\( Y_i \\) is the actual value and \\( \\hat{Y}_i \\) is the predicted value.\n",
        "\n",
        "---\n",
        "\n",
        "### Use Cases:\n",
        "1. Predicting sales based on advertising expenditure.  \n",
        "2. Estimating weight based on height.  \n",
        "3. Predicting temperature based on time of day.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "If \\( \\beta_0 = 2 \\) and \\( \\beta_1 = 3 \\), the equation becomes:  \n",
        "\\[\n",
        "Y = 2 + 3X\n",
        "\\]  \n",
        "For \\( X = 4 \\):  \n",
        "\\[\n",
        "Y = 2 + 3(4) = 14\n",
        "\\]  \n",
        "Thus, the predicted value of \\( Y \\) is 14."
      ],
      "metadata": {
        "id": "QoJoWl6QTH-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What are the key assumptions of Simple Linear Regression ?\n",
        "# **Ans:-**The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "1. **Linearity**: The relationship between the independent variable (\\(X\\)) and dependent variable (\\(Y\\)) is linear.\n",
        "2. **Independence**: Observations are independent of each other.\n",
        "3. **Homoscedasticity**: The variance of residuals is constant across all levels of \\(X\\).\n",
        "4. **Normality**: Residuals are normally distributed.\n",
        "5. **No Multicollinearity**: Applies when multiple predictors are used (not relevant for simple regression)."
      ],
      "metadata": {
        "id": "N75qfI4iTUe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. What does the coefficient m represent in the equation Y=mX+c ?\n",
        "**Ans:-** In the equation \\( Y = mX + c \\), the coefficient \\( m \\) represents the **slope** of the line. It quantifies the change in \\( Y \\) (dependent variable) for a one-unit change in \\( X \\) (independent variable).\n",
        "\n",
        "### Key Points About \\( m \\):\n",
        "1. **Direction of Relationship**:\n",
        "   - If \\( m > 0 \\): \\( Y \\) increases as \\( X \\) increases (positive relationship).\n",
        "   - If \\( m < 0 \\): \\( Y \\) decreases as \\( X \\) increases (negative relationship).\n",
        "   - If \\( m = 0 \\): No relationship; \\( Y \\) remains constant regardless of \\( X \\).\n",
        "\n",
        "2. **Magnitude**:\n",
        "   - The larger the absolute value of \\( m \\), the steeper the slope of the line.\n",
        "   - A smaller \\( |m| \\) indicates a gentler slope.\n",
        "\n",
        "3. **Interpretation**:\n",
        "   - For example, if \\( Y \\) is sales revenue and \\( X \\) is advertising expenditure, \\( m \\) would indicate the additional sales revenue generated for each additional unit of advertising spend.\n",
        "\n",
        "In summary, \\( m \\) determines how \\( Y \\) responds to changes in \\( X \\)."
      ],
      "metadata": {
        "id": "MaqTngbsUCRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "**Ans:-**In the equation \\( Y = mX + c \\), the coefficient \\( c \\) represents the **intercept**, which is the value of \\( Y \\) when the independent variable \\( X \\) is equal to zero.\n",
        "\n",
        "### Key Points About \\( c \\):\n",
        "1. **Position on the Y-Axis**:\n",
        "   - The intercept is the point where the regression line crosses the \\( Y \\)-axis (\\( X = 0 \\)).\n",
        "\n",
        "2. **Interpretation**:\n",
        "   - \\( c \\) provides the baseline value of \\( Y \\) in the absence of any effect from \\( X \\).\n",
        "   - For example, if \\( Y \\) is the total cost and \\( X \\) is the number of units produced, \\( c \\) might represent the fixed costs (cost incurred even if no units are produced).\n",
        "\n",
        "3. **Real-World Meaning**:\n",
        "   - In many cases, the value of \\( c \\) must be interpreted within the context of the problem. If \\( X = 0 \\) is not a meaningful scenario, the intercept may not have a practical interpretation but is still necessary to define the line mathematically.\n",
        "\n",
        "In summary, \\( c \\) sets the starting point for \\( Y \\), anchoring the regression line when \\( X \\) is zero."
      ],
      "metadata": {
        "id": "F-9IXCL_UfTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.  How do we calculate the slope m in Simple Linear Regression\n",
        "**Ans:-** In simple linear regression, the slope (m) represents the change in the dependent variable (y) for every unit change in the independent variable (x). It is calculated using the following formula:\n",
        "\n",
        "m = Σ((x - x̄)(y - ȳ)) / Σ(x - x̄)²\n",
        "\n",
        "where:\n",
        "\n",
        "x and y are the individual data points\n",
        "x̄ and ȳ are the means of x and y, respectively\n",
        "This formula essentially measures the covariance between x and y, normalized by the variance of x. A positive slope indicates a positive relationship between x and y, while a negative slope indicates a negative relationship.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "D30wXObRUujN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "**Ans:-**The purpose of the least squares method in Simple Linear Regression is to find the best-fitting line that minimizes the sum of the squared differences (errors) between the observed values and the predicted values. This method helps in determining the line that minimizes the vertical distance (errors) between the actual data points and the regression line, ensuring the most accurate model for prediction."
      ],
      "metadata": {
        "id": "lk_uLjuPWQTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "**Ans:-** In Simple Linear Regression, the coefficient of determination (\\( R^2 \\)) represents the proportion of the variance in the dependent variable (\\( y \\)) that is explained by the independent variable (\\( x \\)) through the regression model.\n",
        "\n",
        "- \\( R^2 \\) ranges from 0 to 1:\n",
        "  - **\\( R^2 = 1 \\)** indicates that the regression line perfectly fits the data, explaining all the variability in \\( y \\).\n",
        "  - **\\( R^2 = 0 \\)** indicates that the regression line does not explain any of the variability in \\( y \\).\n",
        "  - A higher \\( R^2 \\) value means a better fit of the model to the data, while a lower value suggests a poor fit.\n",
        "\n",
        "Essentially, \\( R^2 \\) tells you how well the independent variable explains the variation in the dependent variable."
      ],
      "metadata": {
        "id": "j1r7eq9WWp2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. What is Multiple Linear Regression\n",
        "**Ans:-** Multiple Linear Regression (MLR) is an extension of Simple Linear Regression that models the relationship between two or more independent variables (predictors) and a dependent variable. It is used to predict the value of the dependent variable based on the values of multiple independent variables.\n",
        "\n",
        "The equation for Multiple Linear Regression is:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( y \\) is the dependent variable.\n",
        "- \\( \\beta_0 \\) is the intercept.\n",
        "- \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients for the independent variables \\( x_1, x_2, \\dots, x_n \\).\n",
        "- \\( \\epsilon \\) is the error term.\n",
        "\n",
        "In MLR, the goal is to find the best-fitting coefficients (\\( \\beta \\)'s) that minimize the difference between the observed and predicted values of \\( y \\), using the least squares method."
      ],
      "metadata": {
        "id": "axBTImJtW4ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. What is the main difference between Simple and Multiple Linear Regression\n",
        "**Ans:-** The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables:\n",
        "\n",
        "- **Simple Linear Regression** involves **one independent variable** (\\(x\\)) to predict the dependent variable (\\(y\\)). The relationship is modeled with a straight line, represented by the equation:\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1 x + \\epsilon\n",
        "  \\]\n",
        "\n",
        "- **Multiple Linear Regression** involves **two or more independent variables** (\\(x_1, x_2, \\dots, x_n\\)) to predict the dependent variable (\\(y\\)). It models the relationship with a hyperplane, represented by the equation:\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
        "  \\]\n",
        "\n",
        "In essence, **Simple Linear Regression** is used when predicting with a single variable, while **Multiple Linear Regression** is used when predicting with multiple variables."
      ],
      "metadata": {
        "id": "6i2Fv4aTXj5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. What are the key assumptions of Multiple Linear Regression.\n",
        "**Ans:-** The key assumptions of Multiple Linear Regression are:\n",
        "\n",
        "1. **Linearity**: The relationship between the dependent variable and each independent variable is linear. This means that changes in the independent variables result in proportional changes in the dependent variable.\n",
        "\n",
        "2. **Independence of Errors**: The residuals (errors) are independent of each other. There should be no correlation between the errors of different observations.\n",
        "\n",
        "3. **Homoscedasticity**: The variance of the errors (residuals) is constant across all levels of the independent variables. The spread of residuals should be roughly the same for all predicted values.\n",
        "\n",
        "4. **Normality of Errors**: The residuals should be approximately normally distributed, especially for hypothesis testing to be valid.\n",
        "\n",
        "5. **No Multicollinearity**: The independent variables should not be highly correlated with each other. High correlation (multicollinearity) can distort the estimation of the regression coefficients and reduce the model’s reliability.\n",
        "\n",
        "6. **No Autocorrelation**: The residuals should not exhibit any systematic patterns over time (especially important in time series data).\n",
        "\n",
        "These assumptions ensure the reliability and accuracy of the Multiple Linear Regression model. Violating these assumptions may affect the quality of predictions and statistical inferences."
      ],
      "metadata": {
        "id": "m4yXldM4YGfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model.\n",
        "**Ans:-** **Heteroscedasticity** refers to a situation in a regression model where the variance of the residuals (errors) is not constant across all levels of the independent variables. In other words, the spread or dispersion of the errors varies as the predicted values (or independent variables) change.\n",
        "\n",
        "### Impact of Heteroscedasticity on Multiple Linear Regression:\n",
        "1. **Inefficient Estimates**: While the regression coefficients (betas) remain unbiased, the presence of heteroscedasticity makes the standard errors of the coefficients unreliable. This can lead to inefficient estimates, meaning the model might not be optimal.\n",
        "   \n",
        "2. **Incorrect Inference**: Since heteroscedasticity affects the standard errors, it can lead to invalid significance tests (such as t-tests) and confidence intervals for the coefficients. This means that you may incorrectly conclude that a predictor is significant or insignificant.\n",
        "\n",
        "3. **Distorted Predictions**: Heteroscedasticity can make the predictions of the model less accurate, especially when it causes large residuals in certain areas of the data.\n",
        "\n",
        "4. **Violation of OLS Assumptions**: Heteroscedasticity violates one of the key assumptions of Ordinary Least Squares (OLS) regression, which assumes constant variance of errors (homoscedasticity). This can lead to suboptimal performance of the model and biased statistical tests.\n",
        "\n",
        "### Detecting Heteroscedasticity:\n",
        "- **Residual Plots**: Plotting the residuals against the fitted values. If the plot shows a pattern (e.g., a funnel shape), it suggests heteroscedasticity.\n",
        "- **Breusch-Pagan Test** or **White Test**: Statistical tests that formally check for heteroscedasticity.\n",
        "\n",
        "### Remedies:\n",
        "- **Transformation of Variables**: Transforming the dependent variable (e.g., using a logarithmic transformation) can often stabilize the variance.\n",
        "- **Weighted Least Squares (WLS)**: A method that adjusts for heteroscedasticity by giving different weights to observations based on the variance of their residuals.\n",
        "- **Robust Standard Errors**: Adjust the standard errors to be more robust to heteroscedasticity without affecting the coefficient estimates.\n",
        "\n",
        "In summary, heteroscedasticity can undermine the reliability of statistical inference in a Multiple Linear Regression model, leading to incorrect conclusions. Detecting and addressing it is essential for accurate modeling."
      ],
      "metadata": {
        "id": "J_sAKv4TYWul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.  How can you improve a Multiple Linear Regression model with high multicollinearity.\n",
        "**Ans:-** To improve a Multiple Linear Regression model with high multicollinearity, you can use the following techniques:\n",
        "\n",
        "### 1. **Remove Highly Correlated Variables**:\n",
        "   - Identify and remove one of the variables in highly correlated pairs or groups. You can use correlation matrices or variance inflation factors (VIF) to identify which variables are causing multicollinearity.\n",
        "\n",
        "### 2. **Combine Variables**:\n",
        "   - Create new features by combining highly correlated variables into a single predictor. This can be done using techniques like **Principal Component Analysis (PCA)** or by simply taking the average or sum of the correlated variables.\n",
        "\n",
        "### 3. **Apply Regularization**:\n",
        "   - **Ridge Regression (L2 regularization)** and **Lasso Regression (L1 regularization)** are techniques that can help manage multicollinearity by adding a penalty term to the cost function. These techniques reduce the magnitude of the coefficients and can shrink some coefficients to zero, helping to eliminate less important predictors.\n",
        "   \n",
        "   - **Ridge Regression**: Reduces the impact of correlated predictors without setting their coefficients to zero.\n",
        "   - **Lasso Regression**: Can eliminate certain variables completely (set coefficients to zero) by applying a stronger penalty.\n",
        "\n",
        "### 4. **Principal Component Analysis (PCA)**:\n",
        "   - PCA transforms the correlated features into a smaller set of uncorrelated components. These components are linear combinations of the original variables and can be used as inputs to the regression model.\n",
        "\n",
        "### 5. **Increase the Sample Size**:\n",
        "   - In some cases, multicollinearity can be less problematic if you have a large enough sample size. Increasing the sample size can help the model distinguish between the correlated predictors more effectively.\n",
        "\n",
        "### 6. **Centered and Scaled Data**:\n",
        "   - Sometimes, scaling the variables (e.g., using standardization or normalization) can help reduce collinearity issues, especially if the variables have different units or scales.\n",
        "\n",
        "### 7. **Remove or Transform Outliers**:\n",
        "   - Outliers can exacerbate multicollinearity by disproportionately influencing the relationships between predictors. Identifying and removing or transforming outliers can sometimes help.\n",
        "\n",
        "### 8. **Domain Knowledge**:\n",
        "   - If you have domain knowledge about which variables are truly important, you can prioritize those predictors and remove less relevant ones, based on their theoretical relevance.\n",
        "\n",
        "### 9. **Check Variance Inflation Factor (VIF)**:\n",
        "   - If VIF values are above a certain threshold (typically 5 or 10), it indicates high multicollinearity. Removing or combining variables with high VIF can improve the model.\n",
        "\n",
        "By addressing multicollinearity, you can make your Multiple Linear Regression model more stable, accurate, and interpretable."
      ],
      "metadata": {
        "id": "SFDzFWD3Ytqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. What are some common techniques for transforming categorical variables for use in regression models\n",
        "**Ans:-** Transforming categorical variables for use in regression models is essential because regression models typically require numeric inputs. Here are some common techniques for converting categorical variables into numerical format:\n",
        "\n",
        "### 1. **One-Hot Encoding (Dummy Variables)**\n",
        "   - **Description**: This technique creates a new binary (0 or 1) variable for each category of the categorical variable. Each category is represented by a new column, and a \"1\" is placed in the corresponding column for that observation, while \"0\" is placed in the other columns.\n",
        "   - **When to Use**: Ideal for nominal categorical variables (no inherent order).\n",
        "   - **Example**: If you have a \"Color\" variable with categories \"Red,\" \"Blue,\" and \"Green,\" it would be transformed into three columns:\n",
        "     - Red: 1 or 0\n",
        "     - Blue: 1 or 0\n",
        "     - Green: 1 or 0\n",
        "\n",
        "   - **Python Code**:\n",
        "     ```python\n",
        "     pd.get_dummies(df['Color'])\n",
        "     ```\n",
        "\n",
        "### 2. **Label Encoding**\n",
        "   - **Description**: Label Encoding assigns an integer to each category of a categorical variable. Each category is replaced by a unique integer based on alphabetical ordering (or specified order).\n",
        "   - **When to Use**: Suitable for ordinal categorical variables (where there is a meaningful order, like \"Low,\" \"Medium,\" \"High\").\n",
        "   - **Example**: For a variable \"Size\" with categories \"Small,\" \"Medium,\" and \"Large,\" the encoding could be:\n",
        "     - Small: 0\n",
        "     - Medium: 1\n",
        "     - Large: 2\n",
        "\n",
        "   - **Python Code**:\n",
        "     ```python\n",
        "     from sklearn.preprocessing import LabelEncoder\n",
        "     le = LabelEncoder()\n",
        "     df['Size_encoded'] = le.fit_transform(df['Size'])\n",
        "     ```\n",
        "\n",
        "### 3. **Ordinal Encoding**\n",
        "   - **Description**: Similar to Label Encoding, but this technique is specifically used for **ordinal** categorical variables where the categories have an inherent order (e.g., \"Low,\" \"Medium,\" \"High\").\n",
        "   - **When to Use**: When there is a clear order in the categories, and the ordinal relationship should be preserved.\n",
        "   - **Example**: For the \"Education\" variable with categories \"High School,\" \"Bachelor's,\" \"Master's,\" \"PhD,\" you might encode them as:\n",
        "     - High School: 1\n",
        "     - Bachelor's: 2\n",
        "     - Master's: 3\n",
        "     - PhD: 4\n",
        "\n",
        "   - **Python Code**:\n",
        "     ```python\n",
        "     education_order = ['High School', 'Bachelor\\'s', 'Master\\'s', 'PhD']\n",
        "     df['Education_encoded'] = pd.Categorical(df['Education'], categories=education_order, ordered=True).codes\n",
        "     ```\n",
        "\n",
        "### 4. **Frequency or Count Encoding**\n",
        "   - **Description**: This method encodes categorical variables based on the frequency or count of each category in the dataset. Each category is replaced by the number of occurrences of that category.\n",
        "   - **When to Use**: Useful for categorical variables with many categories, especially when you don't want to create a large number of binary columns with one-hot encoding.\n",
        "   - **Example**: For the \"City\" variable with categories \"NY,\" \"LA,\" \"SF,\" you could encode them by how many times each city appears in the data.\n",
        "\n",
        "   - **Python Code**:\n",
        "     ```python\n",
        "     city_counts = df['City'].value_counts()\n",
        "     df['City_encoded'] = df['City'].map(city_counts)\n",
        "     ```\n",
        "\n",
        "### 5. **Target Encoding (Mean Encoding)**\n",
        "   - **Description**: This technique replaces each category with the mean of the target variable for that category. It works by calculating the average value of the dependent variable for each category.\n",
        "   - **When to Use**: Useful when there are many categories, and you want to create a continuous numeric representation of each category.\n",
        "   - **Example**: If you have a \"Region\" variable and you're trying to predict house prices (target variable), you could encode the \"Region\" variable with the average house price for each region.\n",
        "\n",
        "   - **Python Code**:\n",
        "     ```python\n",
        "     target_mean = df.groupby('Region')['Price'].mean()\n",
        "     df['Region_encoded'] = df['Region'].map(target_mean)\n",
        "     ```\n",
        "\n",
        "### 6. **Binary Encoding**\n",
        "   - **Description**: A hybrid method that first converts each category into a binary code, then splits each binary digit into separate columns. It reduces the dimensionality compared to one-hot encoding, making it suitable for categorical variables with a large number of categories.\n",
        "   - **When to Use**: When you have high cardinality (many unique categories).\n",
        "   - **Example**: For a \"City\" variable with categories \"A,\" \"B,\" \"C,\" the binary encoding would be:\n",
        "     - A: 00\n",
        "     - B: 01\n",
        "     - C: 10\n",
        "\n",
        "   - **Python Code**:\n",
        "     ```python\n",
        "     from category_encoders import BinaryEncoder\n",
        "     encoder = BinaryEncoder(cols=['City'])\n",
        "     df = encoder.fit_transform(df)\n",
        "     ```\n",
        "\n",
        "### 7. **One-Hot Encoding with a Drop (Avoiding Dummy Variable Trap)**\n",
        "   - **Description**: When applying one-hot encoding, it's common practice to drop one of the columns to avoid the **dummy variable trap** (multicollinearity). This is typically done by using the `drop_first=True` option in Pandas `get_dummies()`.\n",
        "   - **When to Use**: When you need to avoid perfect multicollinearity that could occur if all categories are included as separate binary columns.\n",
        "   \n",
        "   - **Python Code**:\n",
        "     ```python\n",
        "     pd.get_dummies(df['Color'], drop_first=True)\n",
        "     ```\n",
        "\n",
        "### Choosing the Right Method:\n",
        "- **One-Hot Encoding**: Best for nominal categories with no intrinsic order.\n",
        "- **Label Encoding**: Suitable for ordinal categories with an inherent order.\n",
        "- **Target Encoding**: Useful when there are many categories and the categories are correlated with the target.\n",
        "- **Binary Encoding**: A good option for high-cardinality categorical variables.\n",
        "\n",
        "By applying these techniques, you can effectively transform categorical variables into numerical representations suitable for regression models."
      ],
      "metadata": {
        "id": "VDlahq9tZDh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. What is the role of interaction terms in Multiple Linear Regression ?\n",
        "**Ans:-** In **Multiple Linear Regression**, **interaction terms** are used to model the effect of two or more independent variables interacting with each other on the dependent variable. These terms help to capture situations where the relationship between a predictor and the dependent variable is influenced by the value of another predictor.\n",
        "\n",
        "### Role of Interaction Terms:\n",
        "1. **Capturing Combined Effects**:\n",
        "   - When the effect of one independent variable on the dependent variable depends on the value of another independent variable, an interaction term is added to account for this combined effect. Without interaction terms, the model assumes that the relationship between each predictor and the dependent variable is independent of other predictors.\n",
        "\n",
        "2. **Enhancing Model Flexibility**:\n",
        "   - Interaction terms allow the regression model to be more flexible and better fit data where the effect of a predictor is not constant across the levels of other predictors. This is particularly important when the relationship between variables is nonlinear or more complex than what simple additive terms can capture.\n",
        "\n",
        "3. **Improving Predictive Power**:\n",
        "   - By incorporating interaction terms, you can improve the model's ability to make accurate predictions when the relationship between variables is non-linear or when certain combinations of variables have a unique effect on the outcome.\n",
        "\n",
        "### How Interaction Terms Are Formed:\n",
        "- **Interaction between two predictors** \\(x_1\\) and \\(x_2\\) is represented by multiplying them together: \\(x_1 \\times x_2\\).\n",
        "- The regression equation including an interaction term would look like this:\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\times x_2) + \\epsilon\n",
        "  \\]\n",
        "  Here:\n",
        "  - \\(\\beta_0\\) is the intercept.\n",
        "  - \\(\\beta_1, \\beta_2\\) are the coefficients of the individual predictors \\(x_1\\) and \\(x_2\\).\n",
        "  - \\(\\beta_3\\) is the coefficient of the interaction term \\(x_1 \\times x_2\\), which represents the combined effect of both predictors.\n",
        "\n",
        "### Example:\n",
        "Suppose you are modeling the sales of a product based on advertising spending and the price of the product. You might find that the effect of advertising spending on sales is different at different price levels. In this case, an interaction term between advertising spending and price would capture this combined effect.\n",
        "\n",
        "- Without interaction term: \\( \\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{Advertising} + \\beta_2 \\times \\text{Price} \\)\n",
        "- With interaction term: \\( \\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{Advertising} + \\beta_2 \\times \\text{Price} + \\beta_3 \\times (\\text{Advertising} \\times \\text{Price}) \\)\n",
        "\n",
        "### When to Use Interaction Terms:\n",
        "- When you suspect that the effect of one predictor on the dependent variable varies depending on the level of another predictor.\n",
        "- When adding interaction terms improves the model's predictive performance or accuracy, typically evaluated using metrics such as **Adjusted R²**, **AIC**, or **BIC**.\n",
        "\n",
        "### Challenges with Interaction Terms:\n",
        "- **Model Complexity**: Adding too many interaction terms can increase model complexity, which may lead to overfitting, especially with a small dataset.\n",
        "- **Interpretation**: Interaction terms can make the model harder to interpret, especially when there are many predictors and interactions.\n",
        "\n",
        "In summary, **interaction terms** allow the model to capture complex relationships between predictors, improving both the fit and predictive power of the Multiple Linear Regression model when such interactions exist."
      ],
      "metadata": {
        "id": "PPbTahLMZhE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "**Ans:-** In **Simple Linear Regression**, the intercept represents the predicted value of the dependent variable when the independent variable is zero.\n",
        "\n",
        "In **Multiple Linear Regression**, the intercept represents the predicted value of the dependent variable when all independent variables are zero. The interpretation is similar, but it involves multiple predictors, and the intercept might not always be meaningful if having all predictors equal to zero is unrealistic."
      ],
      "metadata": {
        "id": "ekJJqE49aLvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16.  What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "**Ans:-** The **slope** in regression analysis represents the rate of change in the dependent variable for a one-unit change in the independent variable.\n",
        "\n",
        "- **In Simple Linear Regression**: The slope (\\(\\beta_1\\)) indicates how much the dependent variable \\(y\\) changes for each unit increase in the independent variable \\(x\\). If the slope is positive, as \\(x\\) increases, \\(y\\) also increases. If the slope is negative, as \\(x\\) increases, \\(y\\) decreases.\n",
        "\n",
        "  **Significance**: The slope quantifies the relationship between the predictors and the response variable. It shows how sensitive the dependent variable is to changes in the independent variable.\n",
        "\n",
        "- **In Multiple Linear Regression**: Each slope (\\(\\beta_1, \\beta_2, ..., \\beta_n\\)) indicates the change in the dependent variable for a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
        "\n",
        "  **Significance**: The slopes in multiple regression show how each predictor influences the dependent variable, while controlling for the effects of other predictors.\n",
        "\n",
        "### How it affects predictions:\n",
        "- The slope directly affects the predicted value of the dependent variable. A steeper slope means a larger change in \\(y\\) for a given change in \\(x\\), while a flatter slope means a smaller change.\n",
        "- The magnitude and sign of the slope help in predicting how \\(y\\) will behave as the predictor variables change."
      ],
      "metadata": {
        "id": "LRRs9PCQaudZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17.  How does the intercept in a regression model provide context for the relationship between variables.\n",
        "**Ans:-** The **intercept** in a regression model provides the baseline value of the dependent variable when all the independent variables are equal to zero. It helps establish the starting point or reference level for the relationship between the variables.\n",
        "\n",
        "### Contextual Significance:\n",
        "1. **In Simple Linear Regression**:\n",
        "   - The intercept represents the predicted value of the dependent variable when the independent variable is zero.\n",
        "   - Example: If you're predicting sales based on advertising spend, the intercept would represent the expected sales when there is no advertising spend (i.e., zero spend).\n",
        "\n",
        "2. **In Multiple Linear Regression**:\n",
        "   - The intercept represents the predicted value of the dependent variable when all independent variables are zero.\n",
        "   - Example: If you're predicting house price based on square footage and age, the intercept would represent the expected price when both square footage and age are zero (although this scenario might not always be realistic).\n",
        "\n",
        "### How it provides context:\n",
        "- The intercept sets the starting point from which the changes in the dependent variable are measured based on changes in the independent variables.\n",
        "- In many cases, it helps in understanding the baseline outcome or initial value before any independent variables influence the dependent variable.\n"
      ],
      "metadata": {
        "id": "hEGvAVq-a_m7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. What are the limitations of using R² as a sole measure of model performance\n",
        "**Ans:-** While **R² (Coefficient of Determination)** is a commonly used measure of model performance, relying on it as the sole metric has several **limitations**:\n",
        "\n",
        "### 1. **Doesn't Account for Overfitting**:\n",
        "   - A high R² may indicate a good fit to the training data, but it doesn't necessarily mean the model generalizes well to unseen data. Overfitting can occur, where the model captures noise and specific details of the training data rather than the true underlying relationship.\n",
        "\n",
        "### 2. **Insensitive to Model Complexity**:\n",
        "   - R² can increase simply by adding more predictors to the model, even if those predictors don't improve the model's true predictive power. This can lead to an overly complex model that doesn't generalize well.\n",
        "\n",
        "### 3. **Not Useful for Nonlinear Models**:\n",
        "   - R² is primarily designed for linear relationships. It may not effectively capture the performance of nonlinear models, where other metrics like **Adjusted R²**, **AIC**, or **BIC** might be more appropriate.\n",
        "\n",
        "### 4. **Doesn't Capture Predictive Accuracy**:\n",
        "   - A high R² value doesn't necessarily mean that the model provides accurate predictions. It only tells you how well the model fits the data, not how well it performs on new, unseen data.\n",
        "\n",
        "### 5. **Insensitive to Important Outliers**:\n",
        "   - R² can be influenced by extreme outliers, potentially giving a misleading impression of model quality if the data contains significant outliers.\n",
        "\n",
        "### 6. **Interpretation Issues with Small Datasets**:\n",
        "   - For small datasets, R² may not be reliable. It may give misleading results due to the limited data available to assess the model's performance.\n",
        "\n",
        "### 7. **Doesn't Indicate Causality**:\n",
        "   - A high R² value does not imply causality. It only shows the strength of the correlation between predictors and the response variable but does not confirm cause-and-effect relationships.\n",
        "\n",
        "### Alternatives to R²:\n",
        "   - **Adjusted R²**: Accounts for the number of predictors and penalizes the model for unnecessary complexity.\n",
        "   - **RMSE (Root Mean Squared Error)**: Measures the average magnitude of the error, providing insight into predictive accuracy.\n",
        "   - **AIC/BIC**: Helps compare different models and penalizes for overfitting, useful for model selection.\n",
        "   - **Cross-validation**: Provides a better understanding of how the model performs on unseen data, addressing overfitting concerns.\n",
        "\n",
        "In summary, while R² is useful for understanding the fit of a model, it should not be the sole measure of model performance. It's important to consider other metrics, especially for ensuring generalizability and predictive accuracy."
      ],
      "metadata": {
        "id": "l-Y-WMQ-bRB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. How would you interpret a large standard error for a regression coefficient\n",
        "**Ans:-** A **large standard error** for a regression coefficient suggests that the estimate of the coefficient is **uncertain** or **imprecise**. Here's how you would interpret it in the context of a regression analysis:\n",
        "\n",
        "### 1. **High Variability**:\n",
        "   - A large standard error indicates that the coefficient's estimate has high variability. In other words, if you were to collect different samples from the population, the estimated coefficient could vary widely across those samples.\n",
        "\n",
        "### 2. **Less Confidence in the Estimate**:\n",
        "   - A large standard error reduces the precision of the estimated coefficient, making it harder to draw confident conclusions about the true relationship between the predictor and the dependent variable. It implies that there is greater uncertainty about the value of the coefficient.\n",
        "\n",
        "### 3. **Significance Testing**:\n",
        "   - The larger the standard error, the smaller the **t-statistic** for the coefficient, which is calculated as:\n",
        "     \\[\n",
        "     t = \\frac{\\text{Coefficient Estimate}}{\\text{Standard Error}}\n",
        "     \\]\n",
        "   - A smaller t-statistic reduces the likelihood of rejecting the null hypothesis (that the coefficient is zero), making it harder to determine if the predictor has a statistically significant relationship with the dependent variable.\n",
        "\n",
        "### 4. **Possible Causes of Large Standard Error**:\n",
        "   - **Small Sample Size**: A small number of data points can lead to large variability in the coefficient estimates, resulting in a large standard error.\n",
        "   - **Multicollinearity**: If predictors are highly correlated with each other, it can lead to instability in the coefficient estimates and increase the standard error.\n",
        "   - **Weak Relationship**: If the predictor is not strongly related to the dependent variable, the coefficient estimate may have a larger standard error.\n",
        "   - **Model Misspecification**: If the model does not accurately reflect the underlying data, it can cause inaccurate coefficient estimates and inflate the standard error.\n",
        "\n",
        "### 5. **Impact on Interpretation**:\n",
        "   - A large standard error reduces the reliability of the coefficient estimate. It suggests that the true value of the coefficient could vary significantly, and as a result, the predictor might not have a strong or consistent effect on the dependent variable.\n",
        "\n",
        "### Example:\n",
        "If the estimated coefficient for a predictor is 5, and the standard error is 10, the t-statistic would be \\( \\frac{5}{10} = 0.5 \\). This low t-statistic would indicate that the predictor is likely **not statistically significant** at common significance levels (e.g., 0.05), meaning we cannot confidently say that the predictor has a meaningful impact on the dependent variable.\n",
        "\n",
        "### Conclusion:\n",
        "A large standard error suggests that the coefficient estimate is less reliable and might not be significant. It highlights the need for caution in interpreting the results and possibly revisiting the model, improving the sample size, or addressing multicollinearity or model issues."
      ],
      "metadata": {
        "id": "oLAbF93Rbnvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "**Ans:-** ### **Identifying Heteroscedasticity in Residual Plots**:\n",
        "\n",
        "Heteroscedasticity refers to the condition where the variance of the residuals (errors) is not constant across all levels of the independent variable(s). To identify it, you can use **residual plots**:\n",
        "\n",
        "1. **Residual vs. Fitted Value Plot**:\n",
        "   - **What to look for**: In a residual plot, if the residuals fan out or contract as the fitted values increase (forming a pattern such as a funnel shape), it suggests heteroscedasticity.\n",
        "   - **Example**: If you observe that as the fitted values increase, the spread of residuals becomes wider or narrower, this indicates that the variance of the residuals changes with the level of the predictor.\n",
        "\n",
        "2. **Residual vs. Predictor Plot**:\n",
        "   - **What to look for**: Plotting residuals against individual predictors can help detect heteroscedasticity. If the variance of residuals increases or decreases systematically as a predictor changes, this also indicates heteroscedasticity.\n",
        "\n",
        "3. **Scale-Location Plot**:\n",
        "   - **What to look for**: In this plot, the residuals are plotted against the fitted values after applying a square root transformation. A random scatter of points would indicate homoscedasticity, while a pattern (e.g., a curve or spread increasing with fitted values) would indicate heteroscedasticity.\n",
        "\n",
        "4. **Normal Q-Q Plot**:\n",
        "   - **What to look for**: While this plot primarily checks for normality of residuals, deviations from normality can also signal heteroscedasticity, especially if certain ranges of residuals exhibit non-random patterns.\n",
        "\n",
        "### **Why It's Important to Address Heteroscedasticity**:\n",
        "\n",
        "1. **Violation of Assumptions**:\n",
        "   - **Heteroscedasticity violates one of the key assumptions** of linear regression models: constant variance of residuals (homoscedasticity). This can make statistical tests (such as t-tests and F-tests) invalid, leading to unreliable inferences.\n",
        "\n",
        "2. **Inefficient Estimates**:\n",
        "   - While the estimated coefficients (\\(\\beta\\)) remain unbiased in the presence of heteroscedasticity, the **standard errors** of the coefficients may be incorrect. This can lead to **inefficient parameter estimates** and incorrect conclusions about the significance of predictors.\n",
        "\n",
        "3. **Incorrect Confidence Intervals**:\n",
        "   - If heteroscedasticity is present but not addressed, the confidence intervals for the coefficients could be too wide or too narrow, leading to misinterpretations of the model's uncertainty.\n",
        "\n",
        "4. **Impact on Predictions**:\n",
        "   - **Prediction intervals** will be inaccurate. If the variance of residuals increases with the level of the predictor, prediction intervals for high values of the predictor will be too narrow, and for low values, too wide, leading to biased predictions.\n",
        "\n",
        "### **How to Address Heteroscedasticity**:\n",
        "\n",
        "1. **Transformations**:\n",
        "   - Apply transformations to the dependent variable (such as logarithmic or square root transformations) to stabilize the variance of the residuals.\n",
        "   \n",
        "2. **Weighted Least Squares (WLS)**:\n",
        "   - This method adjusts for heteroscedasticity by giving different weights to different observations based on their variance.\n",
        "\n",
        "3. **Robust Standard Errors**:\n",
        "   - Use **heteroscedasticity-robust standard errors** (e.g., Huber-White standard errors) to adjust the standard errors without transforming the data.\n",
        "\n",
        "4. **Re-evaluate Model Specification**:\n",
        "   - Check for omitted variables or incorrect functional form. Adding relevant predictors or changing the model structure may reduce heteroscedasticity.\n",
        "\n",
        "In summary, detecting heteroscedasticity through residual plots is crucial because it helps ensure the validity of the regression model’s estimates and predictions. Addressing it improves the reliability of inferences and model performance."
      ],
      "metadata": {
        "id": "PiKkGU4wcDmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "**Ans:-** If a **Multiple Linear Regression** model has a **high R²** but a **low adjusted R²**, it suggests that **R²** is being inflated due to the inclusion of **additional predictors** in the model, even if those predictors do not provide meaningful explanatory power. Here's what this means:\n",
        "\n",
        "### **Interpretation:**\n",
        "\n",
        "1. **High R²**:\n",
        "   - **R²** represents the proportion of the variance in the dependent variable that is explained by the independent variables. A high R² indicates that the model explains a large portion of the variation in the dependent variable.\n",
        "   - However, R² can **increase** by simply adding more predictors, regardless of whether they are relevant to the outcome. This can create the illusion that the model fits well.\n",
        "\n",
        "2. **Low Adjusted R²**:\n",
        "   - **Adjusted R²** adjusts the R² value for the number of predictors in the model. It penalizes the model for adding irrelevant predictors that do not improve the model's explanatory power.\n",
        "   - If the adjusted R² is significantly lower than R², it suggests that the added predictors are not helping to explain the dependent variable and may be introducing noise or overfitting.\n",
        "\n",
        "### **Implications:**\n",
        "\n",
        "1. **Overfitting**:\n",
        "   - The model may be overfitting the data. It explains a large portion of the variance in the training data (high R²), but it might not generalize well to new, unseen data.\n",
        "   - The low adjusted R² indicates that the model is capturing noise or irrelevant relationships rather than true patterns.\n",
        "\n",
        "2. **Irrelevant Predictors**:\n",
        "   - The presence of irrelevant or weak predictors in the model is causing the high R² but not contributing meaningfully to the model. As a result, adjusted R² penalizes this by showing a lower value.\n",
        "\n",
        "### **Next Steps:**\n",
        "\n",
        "1. **Simplify the Model**:\n",
        "   - Consider removing irrelevant predictors or applying **stepwise regression** or **regularization techniques** like Lasso or Ridge regression, which can help identify and eliminate non-contributing predictors.\n",
        "\n",
        "2. **Focus on Adjusted R²**:\n",
        "   - When comparing models, it's better to rely on adjusted R² instead of R², as it gives a more accurate indication of how well the model generalizes by considering the number of predictors.\n",
        "\n",
        "3. **Cross-Validation**:\n",
        "   - Perform cross-validation to evaluate how well the model performs on new data and detect potential overfitting.\n",
        "\n",
        "### **Conclusion:**\n",
        "A high R² but low adjusted R² suggests that the model may be too complex or includes irrelevant variables, leading to overfitting. Adjusted R² provides a more reliable measure of model fit and is useful for comparing models with different numbers of predictors."
      ],
      "metadata": {
        "id": "7cro1I1GcUGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. Why is it important to scale variables in Multiple Linear Regression\n",
        "**Ans:-**Scaling variables in **Multiple Linear Regression** is important for several reasons, especially when the independent variables (predictors) have different units, ranges, or magnitudes. Here’s why scaling matters:\n",
        "\n",
        "### 1. **Improves Interpretability of Coefficients**:\n",
        "   - In **unscaled data**, the regression coefficients represent the change in the dependent variable for a one-unit change in the corresponding independent variable. If the variables have different units or scales (e.g., one variable in dollars and another in years), interpreting the coefficients becomes difficult.\n",
        "   - **Scaling** transforms all variables to a common scale (usually zero mean and unit variance), allowing you to directly compare the magnitude of the coefficients to see the relative importance of each predictor.\n",
        "\n",
        "### 2. **Prevents Certain Variables from Dominating**:\n",
        "   - Variables with larger scales or higher variance (e.g., income in thousands vs. age in years) can disproportionately influence the regression model, especially when the regression algorithm uses distance metrics (like **gradient descent**). Without scaling, these variables may dominate the learning process, leading to a model that is biased toward them.\n",
        "   - **Scaling** ensures that all variables contribute equally to the model, regardless of their original scale.\n",
        "\n",
        "### 3. **Improves Model Convergence in Regularized Models**:\n",
        "   - In models like **Ridge Regression** or **Lasso Regression** (which use **regularization** to penalize large coefficients), the penalty is dependent on the scale of the variables. If variables are not scaled, the penalty might disproportionately affect variables with smaller scales, making it difficult for the model to converge or leading to biased regularization.\n",
        "   - Scaling ensures that regularization treats all variables on an equal footing.\n",
        "\n",
        "### 4. **Reduces Multicollinearity**:\n",
        "   - **Multicollinearity** occurs when predictor variables are highly correlated with each other. While scaling alone may not eliminate multicollinearity, it can help in **identifying** or **addressing** it by ensuring that the regression coefficients are on a comparable scale, making it easier to interpret their relationships.\n",
        "\n",
        "### 5. **Facilitates Gradient Descent Optimization**:\n",
        "   - When using **gradient descent** to estimate the coefficients, unscaled variables with different magnitudes can cause the gradient descent algorithm to converge slowly or fail to converge. Large differences in scale can lead to slow learning rates, as the algorithm will struggle to optimize the coefficients for variables with larger scales.\n",
        "   - Scaling the variables helps in optimizing the model more efficiently and allows the algorithm to converge faster.\n",
        "\n",
        "### 6. **Distance-Based Models**:\n",
        "   - If you’re using regression models that rely on distance-based metrics (e.g., in **Principal Component Regression** or **K-Nearest Neighbors** for feature selection), unscaled variables can skew the results, as variables with larger ranges will dominate the distance calculations.\n",
        "   - Scaling ensures that each variable contributes equally to these distance-based methods.\n",
        "\n",
        "### **Common Scaling Methods**:\n",
        "\n",
        "- **Standardization (Z-score scaling)**:\n",
        "  \\[\n",
        "  z = \\frac{x - \\mu}{\\sigma}\n",
        "  \\]\n",
        "  This method transforms the variables to have a **mean of 0** and a **standard deviation of 1**, making them comparable in terms of their contribution to the regression model.\n",
        "\n",
        "- **Min-Max Scaling**:\n",
        "  \\[\n",
        "  x_{\\text{scaled}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
        "  \\]\n",
        "  This method scales the values between a fixed range (typically 0 and 1).\n",
        "\n",
        "### **Conclusion**:\n",
        "Scaling variables is crucial in **Multiple Linear Regression** to ensure that all predictors contribute equally to the model, to improve interpretability, and to enhance model convergence. It is particularly important when using regularization or distance-based methods. Without scaling, models may be biased, and interpretation of the regression coefficients may become difficult or misleading."
      ],
      "metadata": {
        "id": "C6HXB3ZtcoTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23. What is polynomial regression\n",
        "**Ans:-** **Polynomial Regression** is a type of regression where the relationship between the independent variable \\( x \\) and the dependent variable \\( y \\) is modeled as an **nth-degree polynomial**. It is used when the relationship is non-linear, capturing curves or higher-order patterns that linear regression cannot.\n",
        "\n",
        "The general form is:\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_n x^n + \\epsilon\n",
        "\\]\n",
        "\n",
        "### **Key Points**:\n",
        "- **Purpose**: Models non-linear relationships.\n",
        "- **Overfitting Risk**: High-degree polynomials can overfit the data.\n",
        "- **Application**: Useful for capturing complex, curvilinear patterns.\n"
      ],
      "metadata": {
        "id": "vxbpNRHBdXCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. How does polynomial regression differ from linear regression\n",
        "**Ans:-****Polynomial Regression** differs from **Linear Regression** in the way the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\) is modeled.\n",
        "\n",
        "### **Key Differences**:\n",
        "\n",
        "1. **Model Type**:\n",
        "   - **Linear Regression**: Models a straight-line relationship between \\(x\\) and \\(y\\).\n",
        "     \\[\n",
        "     y = \\beta_0 + \\beta_1 x + \\epsilon\n",
        "     \\]\n",
        "   - **Polynomial Regression**: Models a non-linear relationship using polynomial terms (e.g., \\(x^2, x^3\\), etc.).\n",
        "     \\[\n",
        "     y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_n x^n + \\epsilon\n",
        "     \\]\n",
        "\n",
        "2. **Degree of Relationship**:\n",
        "   - **Linear Regression**: Assumes a **linear** (straight-line) relationship.\n",
        "   - **Polynomial Regression**: Allows for a **curved** relationship, capturing more complex patterns like quadratic or cubic curves.\n",
        "\n",
        "3. **Flexibility**:\n",
        "   - **Linear Regression**: Less flexible, only captures linear trends.\n",
        "   - **Polynomial Regression**: More flexible, can capture non-linear trends such as U-shaped or S-shaped curves.\n",
        "\n",
        "4. **Overfitting**:\n",
        "   - **Linear Regression**: Less prone to overfitting unless the model is too simple for the data.\n",
        "   - **Polynomial Regression**: More prone to **overfitting** with higher-degree polynomials, as the model can fit noise in the data.\n",
        "\n",
        "5. **Interpretability**:\n",
        "   - **Linear Regression**: Easier to interpret since the relationship is simple and linear.\n",
        "   - **Polynomial Regression**: More complex and harder to interpret as the degree of the polynomial increases.\n",
        "\n",
        "### **Conclusion**:\n",
        "- **Linear Regression** is used when the relationship between variables is linear.\n",
        "- **Polynomial Regression** is used when the relationship is non-linear and can be captured using polynomial terms."
      ],
      "metadata": {
        "id": "HmlQqPoydskE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 25. When is polynomial regression used\n",
        "**Ans:-** **Polynomial Regression** is used when the relationship between the independent variable \\( x \\) and the dependent variable \\( y \\) is **non-linear** but still needs to be modeled within the framework of regression analysis. Here are the main scenarios where polynomial regression is appropriate:\n",
        "\n",
        "### **When to Use Polynomial Regression:**\n",
        "\n",
        "1. **Non-linear Relationships**:\n",
        "   - When the data shows a **curved** or **non-linear** relationship between the independent and dependent variables that cannot be captured by a straight line. For example, the relationship might resemble a quadratic, cubic, or higher-order curve.\n",
        "\n",
        "2. **Curved Patterns**:\n",
        "   - When the data appears to follow a **U-shaped**, **inverted U-shaped**, or **S-shaped** pattern, which linear regression cannot model effectively.\n",
        "\n",
        "3. **Modeling Complex Patterns**:\n",
        "   - When there is a need to capture complex, higher-order interactions between variables. For example, the relationship might change direction (bend or curve) at certain points, like the performance of a product at different stages or times.\n",
        "\n",
        "4. **Smoothing Data**:\n",
        "   - When you want to **smooth noisy data** to fit a curve and reveal underlying patterns, especially when there’s high variance in the data.\n",
        "\n",
        "5. **Handling Small Sample Sizes**:\n",
        "   - If the sample size is small and a higher-degree polynomial can model the data more effectively than a linear model.\n",
        "\n",
        "6. **Improving Model Fit**:\n",
        "   - When a simple linear regression model does not fit the data well, polynomial regression can help by increasing the flexibility of the model, fitting the data more closely.\n",
        "\n",
        "### **Examples of Polynomial Regression Use**:\n",
        "- **Physics**: Modeling the motion of objects, where acceleration changes over time.\n",
        "- **Economics**: Estimating relationships between variables such as income and spending, where the relationship might not be linear.\n",
        "- **Health Sciences**: Modeling the effect of a drug dose where the response increases up to a point and then decreases (e.g., a U-shape).\n",
        "  \n",
        "### **Conclusion**:\n",
        "Polynomial regression is used when the relationship between variables is non-linear and needs to be captured with polynomial terms, providing a more flexible model than linear regression. However, care should be taken to avoid **overfitting** by choosing an appropriate degree for the polynomial."
      ],
      "metadata": {
        "id": "caNrSJ7Vd-Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 26.  What is the general equation for polynomial regression ?\n",
        "**Ans:-** The general equation for **Polynomial Regression** of degree \\( n \\) is:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_n x^n + \\epsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( y \\) is the dependent variable.\n",
        "- \\( x \\) is the independent variable.\n",
        "- \\( \\beta_0, \\beta_1, \\dots, \\beta_n \\) are the coefficients.\n",
        "- \\( x^2, x^3, \\dots, x^n \\) are the polynomial terms.\n",
        "- \\( \\epsilon \\) is the error term.\n",
        "\n",
        "This equation models non-linear relationships by including higher powers of \\( x \\)."
      ],
      "metadata": {
        "id": "fzy1BdOUeToD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 27. Can polynomial regression be applied to multiple variables\n",
        "**Ans:-** Yes, **Polynomial Regression** can be applied to multiple variables, which is known as **Multiple Polynomial Regression**. In this case, the relationship between the dependent variable \\( y \\) and multiple independent variables \\( x_1, x_2, \\dots, x_p \\) is modeled using polynomial terms.\n",
        "\n",
        "### **General Equation for Multiple Polynomial Regression**:\n",
        "The equation for multiple polynomial regression is an extension of the standard polynomial regression, involving interactions and higher powers of multiple variables.\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\beta_{p+1} x_1^2 + \\beta_{p+2} x_2^2 + \\dots + \\beta_{p+n} x_p^n + \\text{interaction terms} + \\epsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( x_1, x_2, \\dots, x_p \\) are the independent variables.\n",
        "- \\( \\beta_0, \\beta_1, \\dots, \\beta_{p+n} \\) are the coefficients.\n",
        "- Higher-order polynomial terms like \\( x_1^2, x_2^2, \\dots \\) and interaction terms such as \\( x_1x_2 \\) can also be included.\n",
        "- \\( \\epsilon \\) is the error term.\n",
        "\n",
        "### **Key Points**:\n",
        "1. **Higher-Dimensional Polynomial Terms**: You can include squared, cubic, or interaction terms for each independent variable.\n",
        "2. **Increased Flexibility**: The model can capture more complex relationships between multiple variables and the target.\n",
        "\n",
        "### **Conclusion**:\n",
        "Polynomial regression can indeed be applied to multiple variables, enabling the modeling of complex relationships with non-linear patterns between multiple predictors and the target variable."
      ],
      "metadata": {
        "id": "_mtHpoOgerdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 28. What are the limitations of polynomial regression?\n",
        "**Ans:-** **Polynomial Regression** has several limitations that should be considered when applying it to data:\n",
        "\n",
        "### 1. **Overfitting**:\n",
        "   - **Issue**: Polynomial regression can easily overfit the data, especially when using high-degree polynomials. The model may fit the training data very closely, capturing noise and fluctuations that do not generalize well to unseen data.\n",
        "   - **Solution**: Use cross-validation, regularization, or limit the degree of the polynomial to avoid overfitting.\n",
        "\n",
        "### 2. **Complexity and Interpretability**:\n",
        "   - **Issue**: As the degree of the polynomial increases, the model becomes more complex and harder to interpret. Understanding the effect of each term (especially higher-degree terms) can become difficult.\n",
        "   - **Solution**: Limit the polynomial degree or use techniques like feature selection to simplify the model.\n",
        "\n",
        "### 3. **Extrapolation Issues**:\n",
        "   - **Issue**: Polynomial regression models can perform poorly when predicting values outside the range of the training data (extrapolation), especially with high-degree polynomials, which may lead to extreme and unrealistic predictions.\n",
        "   - **Solution**: Be cautious when extrapolating and consider other models for extrapolation tasks.\n",
        "\n",
        "### 4. **Multicollinearity**:\n",
        "   - **Issue**: Polynomial features (e.g., \\( x^2, x^3 \\)) can introduce multicollinearity in the model, where the predictor variables are highly correlated with each other. This can lead to unstable coefficient estimates and affect the interpretability of the model.\n",
        "   - **Solution**: Apply techniques like **Principal Component Analysis (PCA)** or **regularization** (e.g., Ridge or Lasso regression) to mitigate multicollinearity.\n",
        "\n",
        "### 5. **Modeling Non-Linearity**:\n",
        "   - **Issue**: While polynomial regression can capture non-linear relationships, it may not always be the best model. Complex relationships may require more advanced techniques like **decision trees**, **support vector machines**, or **neural networks**.\n",
        "   - **Solution**: Consider other models when the relationship is highly complex or when polynomial regression does not work well.\n",
        "\n",
        "### 6. **Poor Performance with Large Data Sets**:\n",
        "   - **Issue**: For large datasets with many features, polynomial regression can become computationally expensive and inefficient, especially when handling higher-degree polynomials.\n",
        "   - **Solution**: Use dimensionality reduction techniques or explore other machine learning algorithms that scale better with large data.\n",
        "\n",
        "### 7. **Assumption of Smoothness**:\n",
        "   - **Issue**: Polynomial regression assumes that the relationship between the dependent and independent variables is smooth. If the data has abrupt changes or discontinuities, polynomial regression may not be suitable.\n",
        "   - **Solution**: Explore non-parametric models like **splines** or **decision trees** if the data exhibits such behavior.\n",
        "\n",
        "### **Conclusion**:\n",
        "While polynomial regression is powerful for modeling non-linear relationships, it has limitations related to overfitting, complexity, extrapolation, multicollinearity, and scalability. Careful consideration and techniques like cross-validation, regularization, or model selection should be applied to mitigate these issues."
      ],
      "metadata": {
        "id": "Xp9Bcu6ze-SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "**Ans:-** When selecting the degree of a polynomial for a regression model, it's important to evaluate how well the model fits the data and avoid overfitting. Here are some methods commonly used to assess model fit:\n",
        "\n",
        "### 1. **Cross-Validation**:\n",
        "   - **What**: Split the dataset into training and validation sets (e.g., using k-fold cross-validation).\n",
        "   - **Why**: Helps to evaluate how well the model generalizes to unseen data, reducing the risk of overfitting.\n",
        "   - **How**: Train the model on the training set and test it on the validation set. Repeat this process for different polynomial degrees and compare the performance.\n",
        "\n",
        "### 2. **Training vs. Testing Error**:\n",
        "   - **What**: Compare the **training error** (error on training data) and **testing error** (error on validation or test data).\n",
        "   - **Why**: If the training error decreases significantly with higher degrees but the testing error increases, it indicates overfitting.\n",
        "   - **How**: Plot training and testing errors against the polynomial degree to visualize the point where the testing error stops decreasing or starts increasing.\n",
        "\n",
        "### 3. **Adjusted R²**:\n",
        "   - **What**: The **Adjusted R²** takes into account the number of predictors (or polynomial terms) in the model, penalizing the use of excessive terms.\n",
        "   - **Why**: Unlike R², which can increase as more terms are added, Adjusted R² provides a more reliable measure of model performance by accounting for the model's complexity.\n",
        "   - **How**: Compare the Adjusted R² values for different polynomial degrees. A large increase in Adjusted R² suggests a good fit, while a sharp decrease indicates overfitting.\n",
        "\n",
        "### 4. **Akaike Information Criterion (AIC)** and **Bayesian Information Criterion (BIC)**:\n",
        "   - **What**: AIC and BIC are statistical measures that evaluate the model's fit while penalizing for complexity (number of parameters).\n",
        "   - **Why**: These criteria help balance goodness of fit and model complexity, helping to prevent overfitting.\n",
        "   - **How**: Lower AIC or BIC values indicate better models. Compare the AIC/BIC for different polynomial degrees.\n",
        "\n",
        "### 5. **Residual Analysis**:\n",
        "   - **What**: Analyze the residuals (the differences between observed and predicted values).\n",
        "   - **Why**: Residual plots can indicate whether the model is appropriate for the data.\n",
        "   - **How**: Plot residuals for different polynomial degrees. Ideally, the residuals should be randomly scattered around zero. If you see patterns (e.g., curvature), it suggests the degree is either too high or too low.\n",
        "\n",
        "### 6. **Mean Squared Error (MSE) or Root Mean Squared Error (RMSE)**:\n",
        "   - **What**: MSE and RMSE measure the average squared difference (or root) between predicted and actual values.\n",
        "   - **Why**: These metrics quantify how well the model fits the data.\n",
        "   - **How**: Compute MSE or RMSE for different polynomial degrees. Lower values indicate a better fit, but be mindful of overfitting with higher-degree polynomials.\n",
        "\n",
        "### 7. **Visual Inspection (Plotting)**:\n",
        "   - **What**: Plot the data points and the model's fitted curve.\n",
        "   - **Why**: Visual inspection can help assess whether the model captures the underlying pattern in the data.\n",
        "   - **How**: Plot the observed data and the fitted curve for different degrees of the polynomial. Select the degree where the curve best matches the data without overfitting.\n",
        "\n",
        "### 8. **Out-of-Sample Error**:\n",
        "   - **What**: Evaluate the model on new, unseen data (a separate test set).\n",
        "   - **Why**: Helps assess how well the model generalizes to data it has not seen before.\n",
        "   - **How**: Test the model on a held-out test set after training on the training data. Compare the prediction error for different polynomial degrees.\n",
        "\n",
        "### **Conclusion**:\n",
        "To select the optimal degree for polynomial regression, it’s important to use a combination of methods like cross-validation, residual analysis, Adjusted R², and error metrics to ensure the model fits well without overfitting. Regularly comparing model performance on both training and testing data is key."
      ],
      "metadata": {
        "id": "gyDYFmeKfP0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30. Why is visualization important in polynomial regression\n",
        "**Ans:-** **Visualization** is crucial in **Polynomial Regression** for several reasons:\n",
        "\n",
        "### 1. **Understanding Model Fit**:\n",
        "   - **What**: Visualization helps you see how well the polynomial regression curve fits the data.\n",
        "   - **Why**: By plotting both the data points and the fitted polynomial curve, you can visually assess whether the model captures the underlying pattern or trends in the data.\n",
        "   - **How**: A good fit will show the curve following the general trend of the data points, while a poor fit will show noticeable deviations.\n",
        "\n",
        "### 2. **Detecting Overfitting**:\n",
        "   - **What**: Visualization makes it easier to spot overfitting, especially when using higher-degree polynomials.\n",
        "   - **Why**: With overfitting, the polynomial curve may excessively twist and turn, closely following all data points, including noise and outliers.\n",
        "   - **How**: A complex curve that fluctuates too much (especially with high-degree polynomials) may indicate overfitting, whereas a simpler curve that captures the general pattern is preferred.\n",
        "\n",
        "### 3. **Choosing the Right Polynomial Degree**:\n",
        "   - **What**: Visualization aids in selecting the appropriate polynomial degree by comparing different models visually.\n",
        "   - **Why**: A low-degree polynomial may underfit, while a high-degree polynomial may overfit. Visualization can help find the \"sweet spot.\"\n",
        "   - **How**: You can plot models with various polynomial degrees and visually inspect which one provides a good balance between fitting the data and avoiding overfitting.\n",
        "\n",
        "### 4. **Assessing the Residuals**:\n",
        "   - **What**: Plotting residuals (the differences between actual and predicted values) helps evaluate how well the model fits the data.\n",
        "   - **Why**: If the residuals appear randomly scattered around zero, it suggests a good fit. If residuals show patterns (e.g., curvature), it indicates the model may not have captured the data's underlying structure.\n",
        "   - **How**: By plotting residuals for different degrees, you can ensure that the model is properly fitting the data.\n",
        "\n",
        "### 5. **Identifying Data Trends and Outliers**:\n",
        "   - **What**: Visualizing the data alongside the fitted polynomial curve can help detect trends, patterns, and outliers that might not be apparent in numerical outputs alone.\n",
        "   - **Why**: Visualizing the data gives a clearer sense of its distribution, identifying any outliers or influential points that could skew the model's performance.\n",
        "   - **How**: By examining the graph, you can spot outliers that the polynomial might not fit well, helping decide if additional techniques are needed.\n",
        "\n",
        "### 6. **Improving Model Interpretation**:\n",
        "   - **What**: Visualization aids in better understanding the relationship between the independent and dependent variables.\n",
        "   - **Why**: A plotted curve illustrates how changes in the predictor variable(s) affect the response variable, making it easier to interpret and communicate the model's behavior.\n",
        "   - **How**: Visualizing polynomial curves for different degrees enables better communication of how the model captures the underlying relationship.\n",
        "\n",
        "### Conclusion:\n",
        "Visualization in polynomial regression is important for evaluating model fit, detecting overfitting, choosing the right degree, and assessing residuals. It provides a clear, intuitive way to understand and improve the model's performance, making it an essential tool for effective model selection and interpretation."
      ],
      "metadata": {
        "id": "Kd-tPeg0feN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 31. How is polynomial regression implemented in Python\n",
        "**Ans:-** Polynomial Regression can be implemented in Python using libraries like **NumPy** and **scikit-learn**. Below is an example of how to implement polynomial regression using **scikit-learn**.\n",
        "\n",
        "### Steps to Implement Polynomial Regression in Python:\n",
        "\n",
        "1. **Import Libraries**:\n",
        "   - You'll need **NumPy** for numerical operations, **matplotlib** for visualization, and **scikit-learn** for creating and fitting the polynomial regression model.\n",
        "\n",
        "2. **Prepare the Data**:\n",
        "   - You'll need a dataset with independent variable(s) (features) and a dependent variable (target).\n",
        "\n",
        "3. **Create Polynomial Features**:\n",
        "   - Using **PolynomialFeatures** from **scikit-learn**, you can create polynomial features from the original data.\n",
        "\n",
        "4. **Fit a Polynomial Regression Model**:\n",
        "   - Use **LinearRegression** from **scikit-learn** to fit the polynomial regression model.\n",
        "\n",
        "5. **Make Predictions**:\n",
        "   - Predict the values using the fitted model.\n",
        "\n",
        "6. **Visualize**:\n",
        "   - Use **matplotlib** to visualize the polynomial regression curve alongside the original data points.\n",
        "\n",
        "\n",
        "\n",
        "### Explanation of the Code:\n",
        "- **Step 1**: We import the necessary libraries for handling arrays, creating plots, and applying machine learning models.\n",
        "- **Step 2**: A simple dataset is created with a quadratic relationship (e.g., \\( y = x^2 \\)).\n",
        "- **Step 3**: We use `PolynomialFeatures` to create polynomial features of the data. In this case, we use degree=2 to generate quadratic features.\n",
        "- **Step 4**: We fit the polynomial regression model using `LinearRegression`.\n",
        "- **Step 5**: We make predictions for the values of \\( y \\) based on the polynomial transformation of \\( X \\).\n",
        "- **Step 6**: We visualize the original data and the fitted polynomial curve using **matplotlib**.\n",
        "\n",
        "### Notes:\n",
        "- You can change the **degree** of the polynomial in the `PolynomialFeatures(degree=2)` step to experiment with different polynomial degrees (e.g., degree=3 for cubic, etc.).\n",
        "- The `LinearRegression` model is used even for polynomial regression because polynomial regression is essentially a linear regression model applied to polynomial features of the data.\n",
        "- The visualization helps you see how well the polynomial curve fits the data.\n",
        "\n",
        "### Output:\n",
        "- The plot will show the original data points and the fitted polynomial regression curve, allowing you to visually assess the fit.\n",
        "\n",
        "This is a basic example, and you can expand it for more complex datasets or higher-degree polynomials."
      ],
      "metadata": {
        "id": "DRPE0ZS1ftNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Step 2: Prepare the Data (example with a simple dataset)\n",
        "# Independent variable (X) - 2D array\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "# Dependent variable (Y)\n",
        "y = np.array([1, 4, 9, 16, 25])\n",
        "\n",
        "# Step 3: Create Polynomial Features\n",
        "# Define the degree of the polynomial (e.g., degree=2 for quadratic)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Step 4: Fit Polynomial Regression Model\n",
        "# Create and fit the linear regression model on the transformed polynomial features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Step 5: Make Predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Step 6: Visualize the Results\n",
        "# Plot the original data points\n",
        "plt.scatter(X, y, color='blue', label='Original Data')\n",
        "# Plot the polynomial regression curve\n",
        "plt.plot(X, y_pred, color='red', label='Polynomial Regression (Degree 2)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "o7_LICOOgLTz",
        "outputId": "e2ed33bd-eefc-49a0-86ba-9f3aef00d5c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
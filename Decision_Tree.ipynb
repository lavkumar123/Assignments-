{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical**"
      ],
      "metadata": {
        "id": "HnKCADKnhQV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a Decision Tree, and how does it work\t?\n",
        "### **Ans:-**A **Decision Tree** is a supervised machine learning algorithm used for classification and regression tasks. It is a tree-like structure where each internal node represents a decision rule based on a feature, each branch represents the outcome of a decision, and each leaf node represents a class label (for classification) or a numerical value (for regression).  \n",
        "\n",
        "### **How It Works:**\n",
        "1. **Start with the Root Node** – The algorithm selects the best feature to split the data based on a criterion such as **Gini impurity**, **Entropy (Information Gain)** (for classification), or **Mean Squared Error (MSE)** (for regression).\n",
        "2. **Splitting the Data** – The dataset is split into smaller subsets based on the selected feature’s values.\n",
        "3. **Recursive Splitting** – The process is repeated for each subset, forming branches, until a stopping condition is met (e.g., maximum depth, minimum samples per node, or pure class labels in a leaf node).\n",
        "4. **Leaf Nodes** – Once the splitting stops, the final nodes (leaf nodes) contain the predicted class labels or regression values.\n",
        "\n"
      ],
      "metadata": {
        "id": "vtLeXPPJWAaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What are impurity measures in Decision Trees ?\n",
        "### **Ans:-** **Impurity Measures in Decision Trees**\n",
        "\n",
        "1. **Gini Impurity**  \n",
        "   - Measures how often a randomly chosen element would be incorrectly classified.  \n",
        "   - The lower the value, the purer the node.  \n",
        "   - Example: If a node contains 80% of class A and 20% of class B, the impurity is **0.32** (lower values are better).  \n",
        "\n",
        "2. **Entropy**  \n",
        "   - Measures the randomness or disorder in the dataset.  \n",
        "   - If all elements belong to the same class, entropy is zero (perfect purity).  \n",
        "   - Example: If a node contains 80% of class A and 20% of class B, the entropy is **0.72** (lower values indicate purer nodes).  \n",
        "\n",
        "3. **Variance (For Regression Trees)**  \n",
        "   - Measures the spread of numerical values in a node.  \n",
        "   - A lower variance means the node contains values that are closer together.  \n",
        "   - Example: If the values in a node are 3, 5, and 7, the variance is **2.67** (higher variance means greater spread).  \n",
        "\n",
        "4. **Mean Squared Error (MSE) (For Regression Trees)**  \n",
        "   - Measures the average squared difference between actual and predicted values.  \n",
        "   - A lower MSE means better accuracy.  \n",
        "   - Example: If actual values are 3, 5, and 7, and predicted values are 4, 6, and 8, the MSE is **1.00** (lower values mean better predictions).  \n"
      ],
      "metadata": {
        "id": "Wb0HwAwTX1g8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. What is the mathematical formula for Gini Impurity\n",
        "### **Ans:-**The **mathematical formula** for **Gini Impurity** is:  \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Gini Impurity = 1 - (p1^2 + p2^2 + ... + pn^2)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `p1, p2, ..., pn` are the probabilities of each class in the dataset.\n",
        "\n",
        "  \n",
        "\n",
        "### **Example Calculation**  \n",
        "If a node contains 80% of class A and 20% of class B:  \n",
        "\n",
        "\\[\n",
        "Gini = 1 - (0.8^2 + 0.2^2)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= 1 - (0.64 + 0.04)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= 1 - 0.68\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= 0.32\n",
        "\\]\n",
        "\n",
        "A **lower Gini impurity** indicates a **purer** node. 🚀"
      ],
      "metadata": {
        "id": "hE7wVChsYrcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What is the mathematical formula for Entropy\n",
        "### **Ans:-** **Here's the mathematical formula for Entropy**\n",
        "\n",
        "```\n",
        "Entropy = - (p1 * log2(p1) + p2 * log2(p2) + ... + pn * log2(pn))\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `p1, p2, ..., pn` are the probabilities of each class in the dataset.\n",
        "- `log2` denotes the logarithm base 2."
      ],
      "metadata": {
        "id": "phxg1tSkaWwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What is Information Gain, and how is it used in Decision Trees\n",
        "### **Ans:-**  **Information Gain in Decision Trees**  \n",
        "\n",
        "**Definition:**  \n",
        "Information Gain (IG) measures the **reduction in entropy** when a dataset is split based on an attribute. It helps **decide which feature to split on** in a decision tree.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Formula**\n",
        "```\n",
        "Information Gain = Entropy(Parent) - Σ [ (Weighted Entropy of each Child) ]\n",
        "```\n",
        "Where:  \n",
        "- `Entropy(Parent)` is the entropy before splitting.  \n",
        "- `Weighted Entropy of each Child` is the entropy of a child node, weighted by the proportion of samples in that node.  \n",
        "- The summation (Σ) runs over all child nodes.  \n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works in Decision Trees**  \n",
        "1. **Calculate Entropy of the Parent Node** (before the split).  \n",
        "2. **Split the Data** based on an attribute.  \n",
        "3. **Calculate the Entropy of Child Nodes** (after the split).  \n",
        "4. **Compute Information Gain** by subtracting the weighted entropy of child nodes from the parent entropy.  \n",
        "5. **Choose the Attribute with the Highest Information Gain** for splitting.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example Calculation**  \n",
        "Suppose we have a dataset with **10 samples**:\n",
        "- **6 are Class A**,  \n",
        "- **4 are Class B**.\n",
        "\n",
        "#### **Step 1: Calculate Parent Entropy**\n",
        "```\n",
        "Entropy(Parent) = - [(6/10) * log2(6/10) + (4/10) * log2(4/10)]\n",
        "                ≈ 0.97\n",
        "```\n",
        "\n",
        "#### **Step 2: Split the Data**\n",
        "Assume splitting by a feature creates two child nodes:\n",
        "1. **Child 1:** 4 samples (3 Class A, 1 Class B)  \n",
        "2. **Child 2:** 6 samples (3 Class A, 3 Class B)  \n",
        "\n",
        "#### **Step 3: Calculate Entropy of Each Child**\n",
        "```\n",
        "Entropy(Child 1) = - [(3/4) * log2(3/4) + (1/4) * log2(1/4)]\n",
        "                 ≈ 0.81\n",
        "```\n",
        "```\n",
        "Entropy(Child 2) = - [(3/6) * log2(3/6) + (3/6) * log2(3/6)]\n",
        "                 = 1.00\n",
        "```\n",
        "\n",
        "#### **Step 4: Compute Weighted Entropy**\n",
        "```\n",
        "Weighted Entropy = (4/10 * 0.81) + (6/10 * 1.00)\n",
        "                 ≈ 0.92\n",
        "```\n",
        "\n",
        "#### **Step 5: Compute Information Gain**\n",
        "```\n",
        "Information Gain = 0.97 - 0.92\n",
        "                 = 0.05\n",
        "```\n",
        "\n",
        "A **higher Information Gain** means the feature is **better for splitting**. The decision tree algorithm picks the feature with the highest IG at each step. 🚀"
      ],
      "metadata": {
        "id": "euDYwgbwbCY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. What is Information Gain, and how is it used in Decision Trees\n",
        "\n",
        "### **Ans:-** **Difference Between Gini Impurity and Entropy**  \n",
        "\n",
        "| **Aspect**        | **Gini Impurity** | **Entropy** |\n",
        "|-------------------|-----------------|------------|\n",
        "| **Definition** | Measures the probability of misclassifying a randomly chosen element. | Measures the amount of randomness (uncertainty) in the dataset. |\n",
        "| **Formula** (Plain Text) | `Gini = 1 - (p1^2 + p2^2 + ... + pn^2)` | `Entropy = - (p1 * log2(p1) + p2 * log2(p2) + ... + pn * log2(pn))` |\n",
        "| **Range** | Between 0 and 0.5 for a binary classification. | Between 0 and 1 for a binary classification. |\n",
        "| **Interpretation** | A lower Gini means purer nodes (less impurity). | A lower Entropy means less randomness (more certainty). |\n",
        "| **Computation Speed** | Faster (no logarithmic calculations). | Slower (involves logarithm calculations). |\n",
        "| **Preference** | Often used in **CART (Classification and Regression Trees)**. | Used in **ID3 and C4.5 decision tree algorithms**. |\n",
        "\n",
        "### **Example Comparison**  \n",
        "If a node has **80% Class A** and **20% Class B**:\n",
        "\n",
        "1. **Gini Impurity Calculation**  \n",
        "   ```\n",
        "   Gini = 1 - (0.8^2 + 0.2^2)  \n",
        "        = 1 - (0.64 + 0.04)  \n",
        "        = 0.32  \n",
        "   ```\n",
        "\n",
        "2. **Entropy Calculation**  \n",
        "   ```\n",
        "   Entropy = - (0.8 * log2(0.8) + 0.2 * log2(0.2))  \n",
        "           ≈ - (0.8 * -0.32 + 0.2 * -2.32)  \n",
        "           ≈ 0.72  \n",
        "   ```\n",
        "\n",
        "### **Which One to Use?**  \n",
        "- **Gini Impurity** is preferred when computation speed is important.  \n",
        "- **Entropy** is preferred when the focus is on information gain and understanding dataset uncertainty.  \n",
        "- In practice, both lead to similar decision trees, so many algorithms default to **Gini Impurity** for efficiency. 🚀"
      ],
      "metadata": {
        "id": "K_f-y3T9b-sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. What is the mathematical explanation behind Decision Trees\n",
        "### **Ans:-** **Mathematical Explanation Behind Decision Trees**  \n",
        "\n",
        "A **Decision Tree** is a supervised learning algorithm that splits a dataset into smaller subsets using a recursive approach based on mathematical criteria like **Gini Impurity, Entropy, or Variance Reduction** (for regression).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Mathematical Process**  \n",
        "\n",
        "#### **1. Selecting the Best Split**  \n",
        "At each step, the algorithm evaluates all possible **features** and **split points** to maximize information gain (for classification) or variance reduction (for regression).  \n",
        "\n",
        "#### **2. Impurity Measures (Classification)**  \n",
        "The goal is to minimize impurity after splitting. Two common measures:  \n",
        "\n",
        "- **Gini Impurity** (Used in CART)  \n",
        "  ```\n",
        "  Gini = 1 - Σ (pi^2)\n",
        "  ```\n",
        "  Where `pi` is the probability of class `i`.  \n",
        "\n",
        "- **Entropy** (Used in ID3, C4.5)  \n",
        "  ```\n",
        "  Entropy = - Σ (pi * log2(pi))\n",
        "  ```\n",
        "  Lower entropy means a purer split.  \n",
        "\n",
        "#### **3. Information Gain (Classification)**  \n",
        "Once we calculate impurity, we measure how much it reduces after a split using **Information Gain (IG)**:  \n",
        "\n",
        "```\n",
        "IG = Entropy(Parent) - Σ (Weighted Entropy of Child Nodes)\n",
        "```\n",
        "\n",
        "The feature with the highest **Information Gain** is chosen for the split.  \n",
        "\n",
        "#### **4. Variance Reduction (Regression Trees)**  \n",
        "For regression, impurity is measured using variance instead of entropy or Gini:  \n",
        "\n",
        "```\n",
        "Variance = (1/n) Σ (yi - mean(y))^2\n",
        "```\n",
        "\n",
        "The feature that minimizes the total variance after splitting is chosen.  \n",
        "\n",
        "#### **5. Recursive Splitting**  \n",
        "The process continues recursively on each child node until:  \n",
        "- A stopping condition is met (e.g., max depth reached, minimum samples per leaf).  \n",
        "- The node becomes **pure** (all samples belong to one class).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example Calculation**  \n",
        "\n",
        "**Dataset Before Split**:  \n",
        "- Class A: 6 samples  \n",
        "- Class B: 4 samples  \n",
        "\n",
        "1. **Calculate Parent Entropy**  \n",
        "   ```\n",
        "   Entropy(Parent) = - [(6/10) log2(6/10) + (4/10) log2(4/10)]\n",
        "                   ≈ 0.97\n",
        "   ```\n",
        "\n",
        "2. **Split the Data**  \n",
        "   Suppose a split creates two child nodes:  \n",
        "   - **Child 1:** 4 samples (3 Class A, 1 Class B)  \n",
        "   - **Child 2:** 6 samples (3 Class A, 3 Class B)  \n",
        "\n",
        "3. **Calculate Child Entropy**  \n",
        "   ```\n",
        "   Entropy(Child 1) = - [(3/4) log2(3/4) + (1/4) log2(1/4)]\n",
        "                    ≈ 0.81\n",
        "   ```\n",
        "   ```\n",
        "   Entropy(Child 2) = - [(3/6) log2(3/6) + (3/6) log2(3/6)]\n",
        "                    = 1.00\n",
        "   ```\n",
        "\n",
        "4. **Compute Weighted Entropy**  \n",
        "   ```\n",
        "   Weighted Entropy = (4/10 * 0.81) + (6/10 * 1.00)\n",
        "                    ≈ 0.92\n",
        "   ```\n",
        "\n",
        "5. **Calculate Information Gain**  \n",
        "   ```\n",
        "   IG = 0.97 - 0.92 = 0.05\n",
        "   ```\n",
        "\n",
        "Since **higher Information Gain is better**, we choose the feature with the highest IG for splitting.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "The mathematical foundation of Decision Trees revolves around:  \n",
        "- **Gini Impurity or Entropy** for classification.  \n",
        "- **Variance Reduction** for regression.  \n",
        "- **Information Gain** to determine the best split.  \n",
        "\n",
        "This recursive process creates an efficient and interpretable model for decision-making. 🚀"
      ],
      "metadata": {
        "id": "qGq16NWTcpdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. What is Pre-Pruning in Decision Trees\n",
        "### **Ans:-** **Pre-Pruning in Decision Trees**  \n",
        "\n",
        "**Definition:**  \n",
        "Pre-Pruning (also called **early stopping**) is a technique used to prevent a Decision Tree from growing too deep, reducing **overfitting**. It stops the tree-building process before it becomes too complex by applying certain constraints.  \n",
        "\n",
        "---\n",
        "\n",
        "### **How Pre-Pruning Works**  \n",
        "Before making a split, the algorithm evaluates whether further splitting is beneficial based on conditions like:  \n",
        "\n",
        "1. **Maximum Depth Constraint**  \n",
        "   - Stops the tree from growing beyond a set depth (`max_depth`).  \n",
        "   - Prevents excessive branching.  \n",
        "\n",
        "2. **Minimum Samples per Node**  \n",
        "   - Requires a node to have at least a minimum number of samples (`min_samples_split`) before splitting.  \n",
        "   - Example: If `min_samples_split = 5`, a node with fewer than 5 samples won’t split.  \n",
        "\n",
        "3. **Minimum Information Gain**  \n",
        "   - Splitting happens only if it increases Information Gain (or decreases impurity) beyond a threshold.  \n",
        "\n",
        "4. **Chi-Square Test (For Categorical Data)**  \n",
        "   - Ensures a split is statistically significant before proceeding.  \n",
        "\n",
        "5. **Maximum Number of Leaf Nodes**  \n",
        "   - Limits the total number of leaf nodes (`max_leaf_nodes`).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example**  \n",
        "Without pre-pruning, a Decision Tree may keep splitting until each leaf contains only **one** sample, causing overfitting.  \n",
        "\n",
        "With pre-pruning:  \n",
        "- If a split **does not reduce impurity significantly**, it is **prevented**.  \n",
        "- If a node has **too few samples**, it remains a leaf.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Pre-Pruning**  \n",
        "✅ Prevents overfitting by **stopping early**.  \n",
        "✅ Reduces training time and improves efficiency.  \n",
        "✅ Creates simpler, more interpretable trees.  \n",
        "\n",
        "### **Disadvantages of Pre-Pruning**  \n",
        "❌ May **underfit** if stopping criteria are too strict.  \n",
        "❌ Choosing the **right threshold** is tricky.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "Pre-pruning **controls tree growth** by applying constraints **before** fully building the tree. It ensures the model remains generalizable and avoids unnecessary complexity. 🚀"
      ],
      "metadata": {
        "id": "j6KZr89xdNMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. What is Post-Pruning in Decision Trees\n",
        "### **Ans:-**  **Post-Pruning in Decision Trees**  \n",
        "\n",
        "#### **Definition:**  \n",
        "Post-Pruning (also called **Pruning after training**) is a technique used to **reduce overfitting** by **trimming branches** from a fully grown Decision Tree. Unlike **Pre-Pruning**, which stops tree growth early, **Post-Pruning allows the tree to grow completely and then removes less significant branches.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **How Post-Pruning Works**  \n",
        "After building the Decision Tree, the pruning process follows these steps:  \n",
        "\n",
        "1. **Grow the Full Tree**  \n",
        "   - The tree is constructed **without constraints**, leading to deep branches that may overfit the training data.  \n",
        "\n",
        "2. **Evaluate Leaf Nodes**  \n",
        "   - The **accuracy** of subtrees (or nodes) is tested on a **validation set**.  \n",
        "\n",
        "3. **Prune Nodes with Low Contribution**  \n",
        "   - If removing a subtree **does not significantly decrease accuracy**, it is **removed**.  \n",
        "   - The node is **converted into a leaf**, using the majority class or average value of the remaining data.  \n",
        "\n",
        "4. **Repeat Until No More Improvement**  \n",
        "   - The process continues until pruning no longer improves validation accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Post-Pruning**  \n",
        "\n",
        "1. **Cost Complexity Pruning (CCP) (Used in CART)**  \n",
        "   - Uses a parameter `α` (alpha) to balance tree complexity and accuracy.  \n",
        "   - Nodes are removed based on a cost function that penalizes overly complex trees.  \n",
        "\n",
        "2. **Reduced Error Pruning (REP) (Used in ID3, C4.5)**  \n",
        "   - Each subtree is replaced by a leaf if the **accuracy on a validation set improves** after pruning.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example**  \n",
        "\n",
        "#### **Before Pruning (Overfitting Example)**  \n",
        "```\n",
        "       Outlook\n",
        "      /       \\\n",
        "   Sunny     Rainy\n",
        "   /   \\       /   \\\n",
        "Hot  Mild  Windy  Calm\n",
        "```\n",
        "\n",
        "#### **After Post-Pruning (Simplified Tree)**  \n",
        "```\n",
        "       Outlook\n",
        "      /       \\\n",
        "   Sunny     Rainy\n",
        "```\n",
        "- If **temperature** and **wind** do not significantly impact predictions, those branches are **removed**, making the model simpler and more generalizable.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Post-Pruning**  \n",
        "✅ Reduces **overfitting**, improving generalization.  \n",
        "✅ Results in a **simpler** and more interpretable tree.  \n",
        "✅ Allows the model to **learn patterns before deciding** which parts to remove.  \n",
        "\n",
        "### **Disadvantages of Post-Pruning**  \n",
        "❌ Requires a **validation set**, increasing data requirements.  \n",
        "❌ **Computationally expensive**, as it needs to evaluate multiple pruned versions.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "Post-Pruning is a powerful way to **fine-tune** a Decision Tree **after training** by removing unnecessary complexity, making it more **accurate on unseen data**. 🚀"
      ],
      "metadata": {
        "id": "-xOWCdVbdm3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. What is the difference between Pre-Pruning and Post-Pruning\n",
        "### **Ans:-** **Difference Between Pre-Pruning and Post-Pruning**  \n",
        "\n",
        "| Feature           | **Pre-Pruning (Early Stopping)** | **Post-Pruning (Pruning After Training)** |\n",
        "|------------------|--------------------------------|--------------------------------|\n",
        "| **Definition**    | Stops tree growth **before** it becomes too deep. | Grows the full tree first, then removes unnecessary branches. |\n",
        "| **When It Happens** | **Before training is complete** (during tree construction). | **After training is complete** (on the fully grown tree). |\n",
        "| **How It Works** | Applies constraints like `max_depth`, `min_samples_split`, and `max_leaf_nodes`. | Evaluates the tree on a **validation set** and prunes nodes that do not improve performance. |\n",
        "| **Purpose** | Prevents overfitting **by stopping early**. | Prevents overfitting **by simplifying the trained model**. |\n",
        "| **Risk** | May cause **underfitting** if stopped too soon. | Less risk of underfitting since the tree is trained fully before pruning. |\n",
        "| **Computational Cost** | Lower, as it prevents unnecessary tree growth. | Higher, as it requires **training a full tree first** and then pruning. |\n",
        "| **Common Techniques** | - Limit tree depth (`max_depth`).  <br> - Require a minimum number of samples to split (`min_samples_split`). | - **Reduced Error Pruning (REP)** (removes nodes if validation accuracy improves). <br> - **Cost Complexity Pruning (CCP)** (removes nodes based on a penalty function). |\n",
        "| **Best Used When** | You want to **control model complexity** from the start. | You want the model to **learn everything first** before deciding what to remove. |\n",
        "\n",
        "### **Conclusion**\n",
        "- **Pre-Pruning** = \"Stop early before overfitting happens.\"  \n",
        "- **Post-Pruning** = \"Grow fully, then trim unnecessary parts.\"  \n",
        "\n",
        "👉 **Post-Pruning is generally preferred**, as it ensures the model learns before reducing complexity. 🚀"
      ],
      "metadata": {
        "id": "ERvGDKvYd3ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. What is a Decision Tree Regressor\n",
        "### **Ans:-**  **Decision Tree Regressor**  \n",
        "\n",
        "A **Decision Tree Regressor** is a type of Decision Tree used for **regression tasks**, meaning it predicts **continuous numerical values** instead of discrete classes (as in classification).  \n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works**  \n",
        "1. **Splitting**:  \n",
        "   - The dataset is recursively split based on **features** to minimize prediction error.  \n",
        "   - Instead of reducing classification impurity (like Gini or Entropy), it minimizes **variance** in the target values.  \n",
        "\n",
        "2. **Leaf Nodes Contain Numerical Values**:  \n",
        "   - Each leaf node represents a **predicted numerical output**, which is typically the **mean** of the target values in that region.  \n",
        "\n",
        "3. **Splitting Criteria (Error Reduction Metrics)**:  \n",
        "   - **Mean Squared Error (MSE)**: Measures variance within each split.  \n",
        "   - **Mean Absolute Error (MAE)**: Uses absolute differences to measure error.  \n",
        "   - **Poisson Deviance**: Used for modeling count-based predictions.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example**  \n",
        "#### **Dataset: Predicting House Prices**  \n",
        "| Square Feet | Bedrooms | Price (in $) |\n",
        "|------------|---------|-------------|\n",
        "| 1000       | 2       | 150,000     |\n",
        "| 1200       | 2       | 180,000     |\n",
        "| 2000       | 3       | 250,000     |\n",
        "| 2200       | 3       | 275,000     |\n",
        "\n",
        "#### **Decision Tree Regressor Output:**  \n",
        "- Splits based on **square feet** or **bedrooms** to group similar house prices.  \n",
        "- Final predictions in each leaf node are the **average price** of grouped houses.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Decision Tree Regressor**  \n",
        "✅ **Easy to understand and interpret**  \n",
        "✅ **Handles both numerical and categorical data**  \n",
        "✅ **Captures non-linear relationships**  \n",
        "\n",
        "### **Disadvantages**  \n",
        "❌ **Prone to overfitting** (without pruning)  \n",
        "❌ **Sensitive to small data variations**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "A **Decision Tree Regressor** predicts continuous values by recursively splitting data and averaging outputs in leaf nodes. It works well for simple problems but requires **pruning or ensemble methods** (like Random Forest) to improve performance. 🚀"
      ],
      "metadata": {
        "id": "XZzS8GEOeGCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. What are the advantages and disadvantages of Decision Trees\n",
        "### **Ans:-**  **Advantages and Disadvantages of Decision Trees**  \n",
        "\n",
        "---\n",
        "\n",
        "### **✅ Advantages**  \n",
        "\n",
        "1. **Easy to Understand & Interpret**  \n",
        "   - Decision Trees are simple and visually intuitive, making them easy to explain to non-technical stakeholders.  \n",
        "\n",
        "2. **Handles Both Numerical & Categorical Data**  \n",
        "   - Unlike some models that require data transformations, Decision Trees naturally work with both types.  \n",
        "\n",
        "3. **Non-Linear Relationships**  \n",
        "   - Can model complex, non-linear relationships without needing feature scaling.  \n",
        "\n",
        "4. **No Need for Feature Scaling**  \n",
        "   - Unlike SVM or logistic regression, Decision Trees don’t require standardization or normalization.  \n",
        "\n",
        "5. **Feature Selection is Built-in**  \n",
        "   - Important features are automatically selected based on how well they split the data.  \n",
        "\n",
        "6. **Handles Missing Values**  \n",
        "   - Can work even if some data points are missing by using surrogate splits.  \n",
        "\n",
        "7. **Works Well with Large Datasets**  \n",
        "   - Efficient for large datasets, especially when using ensemble methods like **Random Forest**.  \n",
        "\n",
        "8. **Fast Training & Prediction**  \n",
        "   - Training and prediction are relatively fast compared to deep learning models.  \n",
        "\n",
        "---\n",
        "\n",
        "### **❌ Disadvantages**  \n",
        "\n",
        "1. **Prone to Overfitting**  \n",
        "   - Deep trees can memorize training data, leading to poor generalization.  \n",
        "   - **Solution:** Use **pruning**, set `max_depth`, or use ensemble methods like **Random Forest**.  \n",
        "\n",
        "2. **Sensitive to Noisy Data**  \n",
        "   - Small variations in training data can create very different trees.  \n",
        "   - **Solution:** Use ensemble techniques like **Bagging** or **Boosting**.  \n",
        "\n",
        "3. **Greedy Algorithm (Locally Optimal Splits)**  \n",
        "   - Splits are based on local decisions, which may not always lead to a globally optimal tree.  \n",
        "   - **Solution:** Use **random forests** or **hyperparameter tuning**.  \n",
        "\n",
        "4. **Biased When Class Imbalance Exists**  \n",
        "   - If one class dominates, the tree may favor it.  \n",
        "   - **Solution:** Use **balanced datasets** or adjust class weights.  \n",
        "\n",
        "5. **Computationally Expensive for Large Feature Spaces**  \n",
        "   - For datasets with **many features**, training can become slow.  \n",
        "   - **Solution:** Perform **feature selection** before training.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "- **Decision Trees are simple, interpretable, and effective for many tasks.**  \n",
        "- However, they **overfit easily** and can be **unstable with small changes** in data.  \n",
        "- **Solution?** Use **Pruning**, **Random Forest**, or **Boosting** to improve performance! 🚀"
      ],
      "metadata": {
        "id": "TOgC2y7Te533"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. How does a Decision Tree handle missing values\n",
        "### **Ans:-** **How a Decision Tree Handles Missing Values**  \n",
        "\n",
        "Decision Trees are quite flexible and can handle missing values in multiple ways. Here are the common approaches:\n",
        "\n",
        "---\n",
        "\n",
        "### **1️⃣ Ignoring Missing Values (Default Behavior)**\n",
        "- If a feature is missing for a data point, the Decision Tree **excludes** that instance while determining splits.  \n",
        "- This works well for small amounts of missing data but reduces training samples.\n",
        "\n",
        "---\n",
        "\n",
        "### **2️⃣ Using Surrogate Splits**\n",
        "- Instead of discarding missing values, Decision Trees use **alternative (surrogate) features** to make splits.  \n",
        "- The tree selects another feature that closely mimics the original split decision.  \n",
        "- **Example:** If `Age` is missing but `Income` is correlated, the tree may use `Income` instead.\n",
        "\n",
        "---\n",
        "\n",
        "### **3️⃣ Assigning the Most Frequent or Mean Value (Imputation)**\n",
        "- For categorical features → **Replace missing values with the most common category**.  \n",
        "- For numerical features → **Replace missing values with the mean or median**.  \n",
        "- This prevents data loss but may introduce bias.\n",
        "\n",
        "---\n",
        "\n",
        "### **4️⃣ Assigning Probabilistic Splits**\n",
        "- If a feature value is missing, the tree distributes the instance **across all possible branches** based on probability.  \n",
        "- The probability is derived from the distribution of existing non-missing values.\n",
        "\n",
        "---\n",
        "\n",
        "### **5️⃣ Using Ensemble Methods (Random Forest)**\n",
        "- **Random Forests** handle missing data better because:\n",
        "  - Each tree in the ensemble sees different subsets of data.\n",
        "  - Missing values have less impact due to averaging multiple models.\n",
        "\n",
        "---\n",
        "\n",
        "### **Which Method to Use?**\n",
        "| Situation | Best Method |\n",
        "|-----------|------------|\n",
        "| Few missing values | Ignore or Impute (Mean/Mode) |\n",
        "| Many missing values in a feature | Remove feature or use Surrogate Splits |\n",
        "| Large dataset with missing values scattered | Probabilistic Splits or Random Forest |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Decision Trees are **naturally robust** to missing values due to **Surrogate Splits** and **Probabilistic Assignments**. However, **preprocessing techniques** like **imputation** and **ensemble learning (Random Forests)** can further improve performance. 🚀"
      ],
      "metadata": {
        "id": "myDGrC-YfSq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. How does a Decision Tree handle categorical features\n",
        "### **Ans:-**  **How a Decision Tree Handles Categorical Features**  \n",
        "\n",
        "Decision Trees can **natively** handle categorical data by splitting on feature values. The way they handle categorical features depends on whether the algorithm is **CART (used in Scikit-learn)** or **ID3/C4.5/C5.0**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1️⃣ One-Hot Encoding (For CART - Scikit-learn)**\n",
        "- The **CART algorithm** (used in Scikit-learn) only supports **numerical splits**, so categorical data is first **converted into numerical form**.  \n",
        "- **One-Hot Encoding** creates separate binary columns for each category (e.g., \"Red\", \"Blue\", \"Green\" → 3 separate columns).  \n",
        "- **Limitation:** Increases the number of features, making the tree **more complex and slower**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **2️⃣ Label Encoding (For CART - Scikit-learn)**\n",
        "- **Assigns numerical labels** to categorical values (`Red = 0, Blue = 1, Green = 2`).  \n",
        "- **Limitation:** The tree may treat numerical values **as ordered**, which is incorrect for nominal categories.\n",
        "\n",
        "---\n",
        "\n",
        "### **3️⃣ Direct Categorical Splitting (For ID3, C4.5, C5.0)**\n",
        "- Algorithms like **ID3, C4.5, C5.0** can **natively handle categorical features**.  \n",
        "- The tree **splits based on unique values of the categorical feature** without converting them into numbers.\n",
        "\n",
        "**Example:**  \n",
        "If a feature **\"Weather\"** has categories **(Sunny, Rainy, Cloudy)**, the tree can **directly** split into three branches:\n",
        "\n",
        "```\n",
        "     Weather\n",
        "     /   |   \\\n",
        " Sunny  Rainy  Cloudy\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4️⃣ Grouping Categories (For High-Cardinality Features)**\n",
        "- If a feature has **too many categories** (e.g., \"City\" with 1000 values), one-hot encoding is inefficient.\n",
        "- Instead, the tree can **group similar categories** based on statistical relationships.\n",
        "\n",
        "---\n",
        "\n",
        "### **Which Method is Best?**\n",
        "| Scenario | Best Approach |\n",
        "|----------|--------------|\n",
        "| Small number of categories (e.g., 3-5) | One-Hot Encoding (if using CART) |\n",
        "| Large number of categories (e.g., 10+) | Grouping or Label Encoding (if order matters) |\n",
        "| If using ID3, C4.5, C5.0 | Direct categorical splits (No encoding needed) |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "- **CART (Scikit-learn)** requires **One-Hot or Label Encoding**.  \n",
        "- **ID3, C4.5, C5.0** can handle categorical features **natively**.  \n",
        "- If a categorical feature has **many unique values**, **grouping** or **feature engineering** is recommended to avoid tree complexity. 🚀"
      ],
      "metadata": {
        "id": "ICRM4_JKfuyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. What are some real-world applications of Decision Trees.\n",
        "### **Ans:-**  **Real-World Applications of Decision Trees**  \n",
        "\n",
        "Decision Trees are widely used across industries due to their simplicity, interpretability, and ability to handle both numerical and categorical data. Here are some key applications:  \n",
        "\n",
        "---\n",
        "\n",
        "### **1️⃣ Healthcare & Medical Diagnosis** 🏥  \n",
        "- **Disease Prediction & Diagnosis** (e.g., predicting diabetes, heart disease, or cancer).  \n",
        "- **Medical Treatment Recommendations** (e.g., choosing the best treatment based on patient symptoms).  \n",
        "- **Patient Risk Assessment** (e.g., determining the likelihood of readmission).  \n",
        "\n",
        "**Example:** A Decision Tree can classify whether a patient has **diabetes** based on features like **age, blood pressure, glucose levels, BMI, etc.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **2️⃣ Finance & Banking** 💰  \n",
        "- **Credit Scoring & Loan Approval** (e.g., deciding if a customer is eligible for a loan based on income, credit history, and past transactions).  \n",
        "- **Fraud Detection** (e.g., identifying unusual credit card transactions).  \n",
        "- **Investment & Stock Market Analysis** (e.g., predicting stock price movements).  \n",
        "\n",
        "**Example:** Banks use Decision Trees to **approve or reject** loan applications based on factors like **credit score, income, debt-to-income ratio, and employment history**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3️⃣ E-Commerce & Retail** 🛒  \n",
        "- **Customer Churn Prediction** (e.g., identifying customers likely to stop using a service).  \n",
        "- **Product Recommendation Systems** (e.g., suggesting products based on user behavior).  \n",
        "- **Price Optimization** (e.g., deciding optimal discounts for different customer segments).  \n",
        "\n",
        "**Example:** An online store can use Decision Trees to **recommend products** based on a customer’s **past purchases and browsing history**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **4️⃣ Marketing & Customer Segmentation** 📢  \n",
        "- **Targeted Advertising** (e.g., predicting which customers are likely to respond to a specific campaign).  \n",
        "- **Lead Scoring** (e.g., identifying high-potential customers for sales teams).  \n",
        "- **Sentiment Analysis** (e.g., analyzing customer feedback to improve service).  \n",
        "\n",
        "**Example:** A telecom company can classify customers into different segments (e.g., **\"High Value\" vs. \"Low Value\"**) and target marketing accordingly.  \n",
        "\n",
        "---\n",
        "\n",
        "### **5️⃣ Manufacturing & Quality Control** 🏭  \n",
        "- **Defect Detection** (e.g., identifying faulty products in an assembly line).  \n",
        "- **Predictive Maintenance** (e.g., predicting when machinery will fail and scheduling maintenance).  \n",
        "- **Supply Chain Optimization** (e.g., determining the best suppliers based on cost and quality).  \n",
        "\n",
        "**Example:** A car manufacturer can use Decision Trees to **detect faulty engine parts** based on sensor data.  \n",
        "\n",
        "---\n",
        "\n",
        "### **6️⃣ Human Resources & Employee Management** 👥  \n",
        "- **Hiring & Recruitment** (e.g., filtering job applicants based on qualifications and experience).  \n",
        "- **Employee Performance Evaluation** (e.g., predicting which employees are likely to be promoted).  \n",
        "- **Attrition Prediction** (e.g., identifying employees at risk of leaving).  \n",
        "\n",
        "**Example:** An HR team can use Decision Trees to **predict employee turnover** based on **salary, work environment, and job satisfaction**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **7️⃣ Education & Student Performance Prediction** 🎓  \n",
        "- **Student Dropout Prediction** (e.g., identifying students at risk of dropping out).  \n",
        "- **Personalized Learning Recommendations** (e.g., suggesting study resources based on performance).  \n",
        "- **Exam Performance Forecasting** (e.g., predicting whether a student will pass or fail an exam).  \n",
        "\n",
        "**Example:** A university can use Decision Trees to **identify struggling students** and provide early intervention.  \n",
        "\n",
        "---\n",
        "\n",
        "### **8️⃣ Fraud Detection & Cybersecurity** 🔒  \n",
        "- **Detecting Spam Emails & Phishing Attacks** (e.g., classifying emails as spam or legitimate).  \n",
        "- **Intrusion Detection Systems (IDS)** (e.g., identifying unauthorized access to a network).  \n",
        "- **Anomaly Detection in Transactions** (e.g., flagging fraudulent bank transactions).  \n",
        "\n",
        "**Example:** A bank can use Decision Trees to **detect fraud** by analyzing **transaction amount, location, and time of purchase**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **9️⃣ Autonomous Vehicles & Traffic Management** 🚗  \n",
        "- **Self-Driving Cars** (e.g., deciding whether to brake, accelerate, or change lanes).  \n",
        "- **Traffic Signal Optimization** (e.g., adjusting signal timings based on traffic conditions).  \n",
        "- **Road Accident Prediction** (e.g., identifying high-risk driving behavior).  \n",
        "\n",
        "**Example:** A self-driving car can use Decision Trees to **decide whether to stop at an intersection based on traffic signals and surrounding vehicles**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **🔹 Conclusion**  \n",
        "Decision Trees are **widely used in various industries** due to their **interpretability, efficiency, and flexibility**. They power applications in **healthcare, finance, marketing, retail, cybersecurity, education, and more**. 🚀"
      ],
      "metadata": {
        "id": "1xdI9C4Fg1_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical**"
      ],
      "metadata": {
        "id": "HgXNVGa7hKcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16.  Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy"
      ],
      "metadata": {
        "id": "RJSWBc6EhiWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree model\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfsncD0Vhthi",
        "outputId": "093dbd00-9119-4153-db57-456e2f2020f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n"
      ],
      "metadata": {
        "id": "7y0gGVeGh_C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini Impurity as the criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2raLN0xsiQeW",
        "outputId": "788d7897-8e41-4e71-8d8d-d56b0392711f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy\n"
      ],
      "metadata": {
        "id": "3lqEDVWJiq4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Entropy as the splitting criterion\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4MAzopEi18I",
        "outputId": "1c176925-1464-4ce4-b8f5-b353dfb32be8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)\n"
      ],
      "metadata": {
        "id": "WftGDIXujKZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKE8SryhjUrK",
        "outputId": "b0180f4b-1c4f-41b2-bd4a-5c5f0c1d3997"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.4952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz"
      ],
      "metadata": {
        "id": "vQMIRVj_jmb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets (optional for visualization)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export the decision tree to a DOT format string\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize the tree using Graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "#graph.render(\"iris_decision_tree\", format=\"png\", cleanup=True)  # Saves the tree visualization as a PNG file\n",
        "graph  # Display the graph in supported environments (e.g., Jupyter Notebook)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8HnA791Hjq9G",
        "outputId": "9000d6ac-dea6-43d6-e8a0-061526207232"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"751pt\" height=\"790pt\"\n viewBox=\"0.00 0.00 751.00 790.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 786)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-786 747,-786 747,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#fdfffd\" stroke=\"black\" d=\"M266,-782C266,-782 131,-782 131,-782 125,-782 119,-776 119,-770 119,-770 119,-711 119,-711 119,-705 125,-699 131,-699 131,-699 266,-699 266,-699 272,-699 278,-705 278,-711 278,-711 278,-770 278,-770 278,-776 272,-782 266,-782\"/>\n<text text-anchor=\"start\" x=\"127\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"163\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"start\" x=\"153.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 120</text>\n<text text-anchor=\"start\" x=\"140.5\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 41, 39]</text>\n<text text-anchor=\"start\" x=\"146\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M167,-655.5C167,-655.5 74,-655.5 74,-655.5 68,-655.5 62,-649.5 62,-643.5 62,-643.5 62,-599.5 62,-599.5 62,-593.5 68,-587.5 74,-587.5 74,-587.5 167,-587.5 167,-587.5 173,-587.5 179,-593.5 179,-599.5 179,-599.5 179,-643.5 179,-643.5 179,-649.5 173,-655.5 167,-655.5\"/>\n<text text-anchor=\"start\" x=\"92.5\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"79.5\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 40</text>\n<text text-anchor=\"start\" x=\"70\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 0, 0]</text>\n<text text-anchor=\"start\" x=\"77\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171.44,-698.91C163.93,-687.65 155.78,-675.42 148.24,-664.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"151.07,-662.05 142.61,-655.67 145.25,-665.93 151.07,-662.05\"/>\n<text text-anchor=\"middle\" x=\"137.71\" y=\"-676.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#f5fef9\" stroke=\"black\" d=\"M344,-663C344,-663 209,-663 209,-663 203,-663 197,-657 197,-651 197,-651 197,-592 197,-592 197,-586 203,-580 209,-580 209,-580 344,-580 344,-580 350,-580 356,-586 356,-592 356,-592 356,-651 356,-651 356,-657 350,-663 344,-663\"/>\n<text text-anchor=\"start\" x=\"205\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.75</text>\n<text text-anchor=\"start\" x=\"248.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"235.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 80</text>\n<text text-anchor=\"start\" x=\"222\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 41, 39]</text>\n<text text-anchor=\"start\" x=\"224\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M225.56,-698.91C231.43,-690.1 237.7,-680.7 243.76,-671.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"246.85,-673.28 249.49,-663.02 241.03,-669.4 246.85,-673.28\"/>\n<text text-anchor=\"middle\" x=\"254.39\" y=\"-683.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3ee684\" stroke=\"black\" d=\"M255.5,-544C255.5,-544 125.5,-544 125.5,-544 119.5,-544 113.5,-538 113.5,-532 113.5,-532 113.5,-473 113.5,-473 113.5,-467 119.5,-461 125.5,-461 125.5,-461 255.5,-461 255.5,-461 261.5,-461 267.5,-467 267.5,-473 267.5,-473 267.5,-532 267.5,-532 267.5,-538 261.5,-544 255.5,-544\"/>\n<text text-anchor=\"start\" x=\"121.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.65</text>\n<text text-anchor=\"start\" x=\"155\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.053</text>\n<text text-anchor=\"start\" x=\"149.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 37</text>\n<text text-anchor=\"start\" x=\"140\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 1]</text>\n<text text-anchor=\"start\" x=\"138\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.66,-579.91C240.13,-571.01 233.14,-561.51 226.39,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.03,-550.01 220.28,-544.02 223.39,-554.15 229.03,-550.01\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9253e8\" stroke=\"black\" d=\"M427.5,-544C427.5,-544 297.5,-544 297.5,-544 291.5,-544 285.5,-538 285.5,-532 285.5,-532 285.5,-473 285.5,-473 285.5,-467 291.5,-461 297.5,-461 297.5,-461 427.5,-461 427.5,-461 433.5,-461 439.5,-467 439.5,-473 439.5,-473 439.5,-532 439.5,-532 439.5,-538 433.5,-544 427.5,-544\"/>\n<text text-anchor=\"start\" x=\"293.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"327\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.206</text>\n<text text-anchor=\"start\" x=\"321.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"start\" x=\"312\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 38]</text>\n<text text-anchor=\"start\" x=\"314\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M306.34,-579.91C312.87,-571.01 319.86,-561.51 326.61,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"329.61,-554.15 332.72,-544.02 323.97,-550.01 329.61,-554.15\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-417.5C109,-417.5 12,-417.5 12,-417.5 6,-417.5 0,-411.5 0,-405.5 0,-405.5 0,-361.5 0,-361.5 0,-355.5 6,-349.5 12,-349.5 12,-349.5 109,-349.5 109,-349.5 115,-349.5 121,-355.5 121,-361.5 121,-361.5 121,-405.5 121,-405.5 121,-411.5 115,-417.5 109,-417.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 36</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M145.4,-460.91C132.28,-449.1 117.96,-436.22 104.89,-424.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.13,-421.76 97.35,-417.67 102.44,-426.96 107.13,-421.76\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-417.5C240,-417.5 151,-417.5 151,-417.5 145,-417.5 139,-411.5 139,-405.5 139,-405.5 139,-361.5 139,-361.5 139,-355.5 145,-349.5 151,-349.5 151,-349.5 240,-349.5 240,-349.5 246,-349.5 252,-355.5 252,-361.5 252,-361.5 252,-405.5 252,-405.5 252,-411.5 246,-417.5 240,-417.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.23,-460.91C192.69,-450.2 193.19,-438.62 193.65,-427.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.15,-427.81 194.08,-417.67 190.16,-427.51 197.15,-427.81\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M426,-425C426,-425 291,-425 291,-425 285,-425 279,-419 279,-413 279,-413 279,-354 279,-354 279,-348 285,-342 291,-342 291,-342 426,-342 426,-342 432,-342 438,-348 438,-354 438,-354 438,-413 438,-413 438,-419 432,-425 426,-425\"/>\n<text text-anchor=\"start\" x=\"287\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"330.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"321\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n<text text-anchor=\"start\" x=\"311.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 4]</text>\n<text text-anchor=\"start\" x=\"306\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M361.11,-460.91C360.83,-452.56 360.52,-443.67 360.23,-435.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"363.73,-434.9 359.89,-425.02 356.73,-435.13 363.73,-434.9\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M603,-425C603,-425 468,-425 468,-425 462,-425 456,-419 456,-413 456,-413 456,-354 456,-354 456,-348 462,-342 468,-342 468,-342 603,-342 603,-342 609,-342 615,-348 615,-354 615,-354 615,-413 615,-413 615,-419 609,-425 603,-425\"/>\n<text text-anchor=\"start\" x=\"464\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"500\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.056</text>\n<text text-anchor=\"start\" x=\"494.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n<text text-anchor=\"start\" x=\"485\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 34]</text>\n<text text-anchor=\"start\" x=\"487\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>6&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M422.52,-460.91C436.88,-451.2 452.31,-440.76 467.02,-430.81\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"469.26,-433.52 475.58,-425.02 465.34,-427.72 469.26,-433.52\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M254,-298.5C254,-298.5 157,-298.5 157,-298.5 151,-298.5 145,-292.5 145,-286.5 145,-286.5 145,-242.5 145,-242.5 145,-236.5 151,-230.5 157,-230.5 157,-230.5 254,-230.5 254,-230.5 260,-230.5 266,-236.5 266,-242.5 266,-242.5 266,-286.5 266,-286.5 266,-292.5 260,-298.5 254,-298.5\"/>\n<text text-anchor=\"start\" x=\"177.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"168\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"158.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"153\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M305.42,-341.91C289.69,-329.88 272.5,-316.73 256.88,-304.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"258.94,-301.96 248.87,-298.67 254.69,-307.52 258.94,-301.96\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M426.5,-306C426.5,-306 296.5,-306 296.5,-306 290.5,-306 284.5,-300 284.5,-294 284.5,-294 284.5,-235 284.5,-235 284.5,-229 290.5,-223 296.5,-223 296.5,-223 426.5,-223 426.5,-223 432.5,-223 438.5,-229 438.5,-235 438.5,-235 438.5,-294 438.5,-294 438.5,-300 432.5,-306 426.5,-306\"/>\n<text text-anchor=\"start\" x=\"292.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"326\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"314.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"313\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M359.54,-341.91C359.75,-333.56 359.98,-324.67 360.2,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"363.7,-316.11 360.46,-306.02 356.71,-315.93 363.7,-316.11\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M256,-179.5C256,-179.5 167,-179.5 167,-179.5 161,-179.5 155,-173.5 155,-167.5 155,-167.5 155,-123.5 155,-123.5 155,-117.5 161,-111.5 167,-111.5 167,-111.5 256,-111.5 256,-111.5 262,-111.5 268,-117.5 268,-123.5 268,-123.5 268,-167.5 268,-167.5 268,-173.5 262,-179.5 256,-179.5\"/>\n<text text-anchor=\"start\" x=\"183.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"174\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"164.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"163\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M309.46,-222.91C294.18,-210.99 277.49,-197.98 262.29,-186.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"264.06,-183.06 254.02,-179.67 259.75,-188.58 264.06,-183.06\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M433,-187C433,-187 298,-187 298,-187 292,-187 286,-181 286,-175 286,-175 286,-116 286,-116 286,-110 292,-104 298,-104 298,-104 433,-104 433,-104 439,-104 445,-110 445,-116 445,-116 445,-175 445,-175 445,-181 439,-187 433,-187\"/>\n<text text-anchor=\"start\" x=\"294\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"330\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"328\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"318.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"313\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M362.89,-222.91C363.17,-214.56 363.48,-205.67 363.77,-197.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"367.27,-197.13 364.11,-187.02 360.27,-196.9 367.27,-197.13\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M347,-68C347,-68 250,-68 250,-68 244,-68 238,-62 238,-56 238,-56 238,-12 238,-12 238,-6 244,0 250,0 250,0 347,0 347,0 353,0 359,-6 359,-12 359,-12 359,-56 359,-56 359,-62 353,-68 347,-68\"/>\n<text text-anchor=\"start\" x=\"270.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"261\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"251.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"246\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M340.55,-103.73C335.19,-94.97 329.52,-85.7 324.14,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"327.08,-75 318.88,-68.3 321.11,-78.66 327.08,-75\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M478,-68C478,-68 389,-68 389,-68 383,-68 377,-62 377,-56 377,-56 377,-12 377,-12 377,-6 383,0 389,0 389,0 478,0 478,0 484,0 490,-6 490,-12 490,-12 490,-56 490,-56 490,-62 484,-68 478,-68\"/>\n<text text-anchor=\"start\" x=\"405.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"396\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"386.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"385\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M390.82,-103.73C396.26,-94.97 402.01,-85.7 407.48,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"410.52,-78.64 412.82,-68.3 404.57,-74.95 410.52,-78.64\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M596,-306C596,-306 471,-306 471,-306 465,-306 459,-300 459,-294 459,-294 459,-235 459,-235 459,-229 465,-223 471,-223 471,-223 596,-223 596,-223 602,-223 608,-229 608,-235 608,-235 608,-294 608,-294 608,-300 602,-306 596,-306\"/>\n<text text-anchor=\"start\" x=\"467\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"498\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"496\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"486.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"485\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>14&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M534.81,-341.91C534.66,-333.56 534.51,-324.67 534.36,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"537.86,-315.96 534.19,-306.02 530.86,-316.08 537.86,-315.96\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M731,-298.5C731,-298.5 638,-298.5 638,-298.5 632,-298.5 626,-292.5 626,-286.5 626,-286.5 626,-242.5 626,-242.5 626,-236.5 632,-230.5 638,-230.5 638,-230.5 731,-230.5 731,-230.5 737,-230.5 743,-236.5 743,-242.5 743,-242.5 743,-286.5 743,-286.5 743,-292.5 737,-298.5 731,-298.5\"/>\n<text text-anchor=\"start\" x=\"656.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"643.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n<text text-anchor=\"start\" x=\"634\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 32]</text>\n<text text-anchor=\"start\" x=\"636\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>14&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M587.19,-341.91C602.37,-329.99 618.95,-316.98 634.04,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"636.56,-307.6 642.26,-298.67 632.24,-302.09 636.56,-307.6\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M575,-179.5C575,-179.5 486,-179.5 486,-179.5 480,-179.5 474,-173.5 474,-167.5 474,-167.5 474,-123.5 474,-123.5 474,-117.5 480,-111.5 486,-111.5 486,-111.5 575,-111.5 575,-111.5 581,-111.5 587,-117.5 587,-123.5 587,-123.5 587,-167.5 587,-167.5 587,-173.5 581,-179.5 575,-179.5\"/>\n<text text-anchor=\"start\" x=\"502.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"493\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"483.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"482\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 15&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>15&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M532.46,-222.91C532.18,-212.2 531.89,-200.62 531.61,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"535.11,-189.57 531.35,-179.67 528.11,-189.75 535.11,-189.57\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M714,-179.5C714,-179.5 617,-179.5 617,-179.5 611,-179.5 605,-173.5 605,-167.5 605,-167.5 605,-123.5 605,-123.5 605,-117.5 611,-111.5 617,-111.5 617,-111.5 714,-111.5 714,-111.5 720,-111.5 726,-117.5 726,-123.5 726,-123.5 726,-167.5 726,-167.5 726,-173.5 720,-179.5 714,-179.5\"/>\n<text text-anchor=\"start\" x=\"637.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"628\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"618.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"613\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 15&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>15&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M579.3,-222.91C592.62,-211.1 607.15,-198.22 620.43,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"622.92,-188.92 628.08,-179.67 618.28,-183.68 622.92,-188.92\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7d6cf62f7d10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree\n"
      ],
      "metadata": {
        "id": "1ajdam0UkXTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with maximum depth of 3 (pruned tree)\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Train a fully grown Decision Tree Classifier (no max_depth constraint)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracy for both models\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_pruned:.2f}\")\n",
        "print(f\"Accuracy of Fully Grown Decision Tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdEy7IAJkph3",
        "outputId": "db0bd3c9-5a37-4c49-ecd0-71023a6b3cc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.00\n",
            "Accuracy of Fully Grown Decision Tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22.  Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree\n"
      ],
      "metadata": {
        "id": "9t1eG9WBk3bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer Wisconsin dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_min_samples.fit(X_train, y_train)\n",
        "y_pred_min_samples = clf_min_samples.predict(X_test)\n",
        "accuracy_min_samples = accuracy_score(y_test, y_pred_min_samples)\n",
        "\n",
        "# Train a default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the accuracies for both models\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_min_samples:.2f}\")\n",
        "print(f\"Accuracy with default settings: {accuracy_default:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re3wuph2lJMZ",
        "outputId": "0e556063-0f87-4d26-c49d-35e97ab70b86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with min_samples_split=5: 0.94\n",
            "Accuracy with default settings: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data\n"
      ],
      "metadata": {
        "id": "9PoliF5TlOz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------------------------\n",
        "# Model with unscaled data\n",
        "# -------------------------\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy with unscaled data: {accuracy_unscaled:.2f}\")\n",
        "\n",
        "# ------------------------------------\n",
        "# Model with scaled data (StandardScaler)\n",
        "# ------------------------------------\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the classifier on scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaled data: {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TizZkD4XlbRx",
        "outputId": "116cb14b-d541-4336-a62f-f4135cc8b4a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with unscaled data: 1.00\n",
            "Accuracy with scaled data: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass  classification\n"
      ],
      "metadata": {
        "id": "T1AbuGqilto5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Wrap the classifier with OneVsRestClassifier for OvR strategy\n",
        "ovr_classifier = OneVsRestClassifier(dt_classifier)\n",
        "\n",
        "# Train the One-vs-Rest classifier on the training data\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-Rest Decision Tree Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_7CEcBsmJSp",
        "outputId": "5097418d-dfb7-42ab-8f2d-2baa270429e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-Rest Decision Tree Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores"
      ],
      "metadata": {
        "id": "auHVZK4YmMrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display the feature importance scores\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3nxxr-tmWQ_",
        "outputId": "24c87473-47a0-445a-c96f-a11eae41b98b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree\n"
      ],
      "metadata": {
        "id": "KykvUiL5mu7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor with max_depth=5\n",
        "regressor_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor_limited.fit(X_train, y_train)\n",
        "y_pred_limited = regressor_limited.predict(X_test)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "\n",
        "# Train an unrestricted Decision Tree Regressor (no max_depth limit)\n",
        "regressor_unrestricted = DecisionTreeRegressor(random_state=42)\n",
        "regressor_unrestricted.fit(X_train, y_train)\n",
        "y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Compare the performance\n",
        "print(\"Decision Tree Regressor Performance:\")\n",
        "print(f\"  MSE with max_depth=5: {mse_limited:.4f}\")\n",
        "print(f\"  MSE with unrestricted tree: {mse_unrestricted:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3XPgfG3m_k0",
        "outputId": "d274b5d6-7247-4e5d-d9ad-84f39eb42932"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor Performance:\n",
            "  MSE with max_depth=5: 0.5245\n",
            "  MSE with unrestricted tree: 0.4952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy\n"
      ],
      "metadata": {
        "id": "_IfXU4kYne_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris data and split\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an unpruned tree and get CCP path\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas[:-1]  # Exclude the maximum alpha\n",
        "\n",
        "# Train trees for each alpha and record test accuracy\n",
        "test_scores = []\n",
        "for alpha in ccp_alphas:\n",
        "    clf_alpha = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    clf_alpha.fit(X_train, y_train)\n",
        "    test_scores.append(accuracy_score(y_test, clf_alpha.predict(X_test)))\n",
        "\n",
        "# Plot test accuracy vs ccp_alpha\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ccp_alphas, test_scores, marker='o', drawstyle=\"steps-post\")\n",
        "plt.xlabel(\"ccp_alpha\")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"CCP: Test Accuracy vs. Alpha\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "iPflKia8oFSa",
        "outputId": "3dfc0f02-fbe2-4ec7-9af9-8fc243944909"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,Recall, and F1-Score\n"
      ],
      "metadata": {
        "id": "J9TmFqxkoNAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall    = recall_score(y_test, y_pred, average='weighted')\n",
        "f1        = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Performance Metrics:\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1-Score:  {f1:.2f}\")\n",
        "\n",
        "# Alternatively, print a detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8QKVzEHoZcL",
        "outputId": "b1c45200-97cf-4ce1-ab9a-f2d762acce5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics:\n",
            "Precision: 1.00\n",
            "Recall:    1.00\n",
            "F1-Score:  1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 29.  Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "1b6q8eHroq9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "-hH-MfSUoqTZ",
        "outputId": "19de245d-2cc6-482f-9bbc-52a3b49660a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30.  Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n"
      ],
      "metadata": {
        "id": "7SfHACYbpYAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for max_depth and min_samples_split\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(dt_clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display the best parameters and best score from GridSearchCV\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best estimator on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DODV7hE1pdiT",
        "outputId": "f4924a02-a3dd-4a5d-e060-a3b8ce74bab8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Best Cross-Validation Accuracy: 0.9416666666666668\n",
            "\n",
            "Classification Report on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}